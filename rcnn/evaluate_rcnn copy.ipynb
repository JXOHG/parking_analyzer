{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNN Model Evaluation\n",
    "\n",
    "Comprehensive evaluation of Faster R-CNN model performance on parking lot detection task.\n",
    "\n",
    "**Metrics Evaluated:**\n",
    "- Object Detection: mAP@0.5, mAP@0.5:0.95, Precision, Recall\n",
    "- Counting Accuracy: Exact Match Accuracy, Mean Absolute Error (MAE)\n",
    "- Inference Speed: Frames Per Second (FPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RUN_DIR = Path('./runs/rcnn/')\n",
    "DATA_DIR = Path('./data/')\n",
    "MODEL_CHECKPOINT = RUN_DIR / 'latest.pt'\n",
    "TEST_JSON = DATA_DIR / 'test.json'\n",
    "TEST_IMG_DIR = DATA_DIR / 'test'\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 0\n",
    "SCORE_THRESHOLD = 0.5\n",
    "\n",
    "if not MODEL_CHECKPOINT.exists():\n",
    "    raise FileNotFoundError(f\"Model checkpoint not found: {MODEL_CHECKPOINT}\")\n",
    "if not TEST_JSON.exists():\n",
    "    raise FileNotFoundError(f\"Test annotations not found: {TEST_JSON}\")\n",
    "if not TEST_IMG_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Test images not found: {TEST_IMG_DIR}\")\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Model: {MODEL_CHECKPOINT}\")\n",
    "print(f\"Data: {TEST_JSON}\")\n",
    "print(f\"Config: batch={BATCH_SIZE}, score_thresh={SCORE_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes, anchor_sizes=None):\n",
    "    try:\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=None)\n",
    "    except AttributeError:\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None)\n",
    "    \n",
    "    if anchor_sizes is not None:\n",
    "        aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "        anchor_generator = AnchorGenerator(sizes=anchor_sizes, aspect_ratios=aspect_ratios)\n",
    "        model.rpn.anchor_generator = anchor_generator\n",
    "    \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "class COCODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, json_file, img_dir, transforms=None):\n",
    "        from pycocotools.coco import COCO\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.transforms = transforms\n",
    "        self.coco = COCO(str(json_file))\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        coco_anns = self.coco.loadAnns(ann_ids)\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = self.img_dir / img_info['file_name']\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        boxes, labels = [], []\n",
    "        for ann in coco_anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            if w <= 0 or h <= 0: continue\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann['category_id'])\n",
    "        \n",
    "        target = {\n",
    "            \"boxes\": torch.as_tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4), dtype=torch.float32),\n",
    "            \"labels\": torch.as_tensor(labels, dtype=torch.int64) if labels else torch.zeros(0, dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([img_id])\n",
    "        }\n",
    "        \n",
    "        if self.transforms:\n",
    "            img_tensor = self.transforms(img)\n",
    "        \n",
    "        return img_tensor, target, img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "def get_transform():\n",
    "    return torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_file = RUN_DIR / 'args.json'\n",
    "anchor_sizes = None\n",
    "if args_file.exists():\n",
    "    with open(args_file, 'r') as f:\n",
    "        train_args = json.load(f)\n",
    "    if train_args.get('anchor_sizes'):\n",
    "        anchor_sizes = tuple(tuple([int(s)]) for s in train_args['anchor_sizes'].split(','))\n",
    "        print(f\"Using custom anchor sizes: {anchor_sizes}\")\n",
    "\n",
    "model = get_model(num_classes=NUM_CLASSES, anchor_sizes=anchor_sizes)\n",
    "checkpoint = torch.load(MODEL_CHECKPOINT, map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "test_dataset = COCODataset(TEST_JSON, TEST_IMG_DIR, transforms=get_transform())\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "print(f\"Test dataset: {len(test_dataset)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Object Detection Metrics (mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_coco(model, data_loader, device, coco_gt):\n",
    "    from pycocotools.cocoeval import COCOeval\n",
    "    \n",
    "    model.eval()\n",
    "    coco_results = []\n",
    "    \n",
    "    for images, targets, _ in tqdm(data_loader, desc=\"Running detection\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        outputs = model(images)\n",
    "        \n",
    "        for target, output in zip(targets, outputs):\n",
    "            image_id = target['image_id'].item()\n",
    "            boxes = output['boxes'].cpu().numpy()\n",
    "            scores = output['scores'].cpu().numpy()\n",
    "            labels = output['labels'].cpu().numpy()\n",
    "            \n",
    "            mask = scores > 0.05\n",
    "            boxes, scores, labels = boxes[mask], scores[mask], labels[mask]\n",
    "            \n",
    "            for box, score, label in zip(boxes, scores, labels):\n",
    "                if label == 0: continue\n",
    "                x1, y1, x2, y2 = box\n",
    "                w, h = x2 - x1, y2 - y1\n",
    "                if w <= 0 or h <= 0: continue\n",
    "                \n",
    "                coco_results.append({\n",
    "                    'image_id': int(image_id),\n",
    "                    'category_id': 1,\n",
    "                    'bbox': [float(x1), float(y1), float(w), float(h)],\n",
    "                    'score': float(score)\n",
    "                })\n",
    "                \n",
    "    if not coco_results:\n",
    "        print(\"No predictions made\")\n",
    "        return None\n",
    "\n",
    "    if 'info' not in coco_gt.dataset:\n",
    "        coco_gt.dataset['info'] = []\n",
    "        \n",
    "    coco_dt = coco_gt.loadRes(coco_results)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "    \n",
    "    # Extract metrics in same format as YOLOv9\n",
    "    return {\n",
    "        'map_50_95': coco_eval.stats[0],\n",
    "        'map_50': coco_eval.stats[1],\n",
    "        'precision': coco_eval.stats[0],  # Using mAP as proxy for precision\n",
    "        'recall': coco_eval.stats[8]      # AR@100\n",
    "    }\n",
    "\n",
    "bbox_metrics = evaluate_coco(model, test_loader, DEVICE, test_dataset.coco)\n",
    "\n",
    "print(\"\\nDetection Metrics:\")\n",
    "if bbox_metrics:\n",
    "    for key, val in bbox_metrics.items():\n",
    "        print(f\"  {key}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Counting Metrics (Accuracy & MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def calculate_counting_metrics(model, data_loader, device, score_threshold):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for images, targets, _ in tqdm(data_loader, desc=\"Calculating counting metrics\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        outputs = model(images)\n",
    "        \n",
    "        for i in range(len(images)):\n",
    "            gt_count = len(targets[i]['labels'])\n",
    "            pred_scores = outputs[i]['scores']\n",
    "            pred_count = (pred_scores > score_threshold).sum().item()\n",
    "            \n",
    "            results.append({\n",
    "                'gt_count': gt_count,\n",
    "                'pred_count': pred_count,\n",
    "                'difference': pred_count - gt_count,\n",
    "                'abs_error': abs(pred_count - gt_count)\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    correct_counts = (df['difference'] == 0).sum()\n",
    "    total_images = len(df)\n",
    "    exact_accuracy = (correct_counts / total_images) * 100\n",
    "    mae = df['abs_error'].mean()\n",
    "    \n",
    "    return {'count_accuracy': exact_accuracy, 'count_mae': mae}, df\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_fps(model, data_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Warm-up\n",
    "    for i, (images, _, _) in enumerate(data_loader):\n",
    "        if i >= 3: break\n",
    "        images = [img.to(device) for img in images]\n",
    "        _ = model(images)\n",
    "\n",
    "    total_time, num_images = 0, 0\n",
    "    for images, _, _ in tqdm(data_loader, desc=\"Measuring inference speed\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        start_time = time.time()\n",
    "        _ = model(images)\n",
    "        total_time += (time.time() - start_time)\n",
    "        num_images += len(images)\n",
    "    \n",
    "    return num_images / total_time if total_time > 0 else 0\n",
    "\n",
    "counting_metrics, count_df = calculate_counting_metrics(model, test_loader, DEVICE, SCORE_THRESHOLD)\n",
    "fps = calculate_fps(model, test_loader, DEVICE)\n",
    "\n",
    "print(\"\\nCounting Metrics:\")\n",
    "print(f\"  Exact Accuracy: {counting_metrics['count_accuracy']:.2f}%\")\n",
    "print(f\"  MAE: {counting_metrics['count_mae']:.4f}\")\n",
    "print(f\"\\nInference Speed: {fps:.2f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Count Analysis Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Ground Truth vs Predicted scatter plot\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(count_df['gt_count'], count_df['pred_count'], alpha=0.6, s=50)\n",
    "max_val = max(count_df['gt_count'].max(), count_df['pred_count'].max())\n",
    "ax1.plot([0, max_val], [0, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "ax1.set_xlabel('Ground Truth Count', fontsize=12)\n",
    "ax1.set_ylabel('Predicted Count', fontsize=12)\n",
    "ax1.set_title('Ground Truth vs Predicted Counts', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Error distribution histogram\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(count_df['difference'], bins=30, edgecolor='black', alpha=0.7)\n",
    "ax2.axvline(x=0, color='r', linestyle='--', linewidth=2, label='Zero Error')\n",
    "ax2.set_xlabel('Prediction Error (Pred - GT)', fontsize=12)\n",
    "ax2.set_ylabel('Frequency', fontsize=12)\n",
    "ax2.set_title('Distribution of Prediction Errors', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Count distribution comparison\n",
    "ax3 = axes[1, 0]\n",
    "bins = np.arange(0, max(count_df['gt_count'].max(), count_df['pred_count'].max()) + 2, 1)\n",
    "ax3.hist(count_df['gt_count'], bins=bins, alpha=0.5, label='Ground Truth', edgecolor='black')\n",
    "ax3.hist(count_df['pred_count'], bins=bins, alpha=0.5, label='Predicted', edgecolor='black')\n",
    "ax3.set_xlabel('Count', fontsize=12)\n",
    "ax3.set_ylabel('Frequency', fontsize=12)\n",
    "ax3.set_title('Distribution of Counts', fontsize=14, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Sample-by-sample comparison (first 30 images)\n",
    "ax4 = axes[1, 1]\n",
    "n_samples = min(30, len(count_df))\n",
    "x = np.arange(n_samples)\n",
    "width = 0.35\n",
    "ax4.bar(x - width/2, count_df['gt_count'].iloc[:n_samples], width, label='Ground Truth', alpha=0.8)\n",
    "ax4.bar(x + width/2, count_df['pred_count'].iloc[:n_samples], width, label='Predicted', alpha=0.8)\n",
    "ax4.set_xlabel('Image Index', fontsize=12)\n",
    "ax4.set_ylabel('Count', fontsize=12)\n",
    "ax4.set_title(f'Count Comparison (First {n_samples} Images)', fontsize=14, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nCount Analysis Statistics:\")\n",
    "print(f\"  Total images: {len(count_df)}\")\n",
    "print(f\"  Exact matches: {(count_df['difference'] == 0).sum()} ({(count_df['difference'] == 0).sum()/len(count_df)*100:.2f}%)\")\n",
    "print(f\"  Mean GT count: {count_df['gt_count'].mean():.2f}\")\n",
    "print(f\"  Mean Pred count: {count_df['pred_count'].mean():.2f}\")\n",
    "print(f\"  GT count range: {count_df['gt_count'].min()} - {count_df['gt_count'].max()}\")\n",
    "print(f\"  Pred count range: {count_df['pred_count'].min()} - {count_df['pred_count'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"      Faster R-CNN Performance Summary\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if bbox_metrics:\n",
    "    print(f\" mAP@0.5:0.95:        {bbox_metrics.get('map_50_95', 0):.4f}\")\n",
    "    print(f\" mAP@0.5:             {bbox_metrics.get('map_50', 0):.4f}\")\n",
    "    print(f\" Precision:           {bbox_metrics.get('precision', 0):.4f}\")\n",
    "    print(f\" Recall:              {bbox_metrics.get('recall', 0):.4f}\")\n",
    "else:\n",
    "    print(\" Detection metrics unavailable\")\n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "if counting_metrics:\n",
    "    print(f\" Count Accuracy:      {counting_metrics.get('count_accuracy', 0):.2f}%\")\n",
    "    print(f\" Count MAE:           {counting_metrics.get('count_mae', 0):.4f}\")\n",
    "else:\n",
    "    print(\" Counting metrics unavailable\")\n",
    "    \n",
    "print(\"-\"*50)\n",
    "\n",
    "print(f\" Inference Speed:     {fps:.2f} FPS\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image, boxes, labels, color, text_prefix=''):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 15)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    for box, label in zip(boxes, labels):\n",
    "        draw.rectangle(list(box), outline=color, width=3)\n",
    "        text = f\"{text_prefix}{label}\"\n",
    "        text_size = draw.textbbox((0,0), text, font=font)\n",
    "        text_width = text_size[2] - text_size[0]\n",
    "        text_height = text_size[3] - text_size[1]\n",
    "        draw.rectangle([box[0], box[1] - text_height, box[0] + text_width, box[1]], fill=color)\n",
    "        draw.text((box[0], box[1] - text_height), text, fill=\"white\", font=font)\n",
    "\n",
    "def visualize_predictions(model, dataset, num_samples=5, score_threshold=0.5):\n",
    "    indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        img_tensor, target, original_img = dataset[idx]\n",
    "        \n",
    "        # Ground Truth\n",
    "        gt_img = original_img.copy()\n",
    "        gt_boxes = target['boxes'].numpy()\n",
    "        gt_count = len(gt_boxes)\n",
    "        gt_labels = ['car'] * gt_count\n",
    "        draw_boxes(gt_img, gt_boxes, gt_labels, color='green')\n",
    "        \n",
    "        # Prediction\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            prediction = model([img_tensor.to(DEVICE)])[0]\n",
    "        \n",
    "        pred_img = original_img.copy()\n",
    "        boxes = prediction['boxes'].cpu().numpy()\n",
    "        scores = prediction['scores'].cpu().numpy()\n",
    "        \n",
    "        mask = scores > score_threshold\n",
    "        boxes, scores = boxes[mask], scores[mask]\n",
    "        pred_count = len(scores)\n",
    "        pred_labels = [f\"{s:.2f}\" for s in scores]\n",
    "        draw_boxes(pred_img, boxes, pred_labels, color='red', text_prefix='car: ')\n",
    "        \n",
    "        # Display\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        ax1.imshow(gt_img)\n",
    "        ax1.set_title(f'Ground Truth (Count: {gt_count})', fontsize=14)\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        ax2.imshow(pred_img)\n",
    "        ax2.set_title(f'Prediction (Count: {pred_count})', fontsize=14)\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "visualize_predictions(model, test_dataset, num_samples=5, score_threshold=SCORE_THRESHOLD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
