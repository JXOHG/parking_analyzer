{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNN Training Script - Version 3 (FIXED)\n",
    "\n",
    "## Critical Fixes Applied:\n",
    "\n",
    "### 1. ‚úÖ Image Normalization Bug (CRITICAL)\n",
    "- **Issue**: Previous versions normalized images with ImageNet stats\n",
    "- **Fix**: Removed normalization, images now scaled to [0, 255] range\n",
    "- **Impact**: Faster R-CNN expects unnormalized images - this was causing zero AP!\n",
    "\n",
    "### 2. ‚úÖ Data Augmentation\n",
    "- **Issue**: No augmentation, limiting model generalization\n",
    "- **Fix**: Added RandomHorizontalFlip with proper bounding box transformation\n",
    "- **Impact**: Better model robustness and performance\n",
    "\n",
    "### 3. ‚úÖ Hyperparameters\n",
    "- **Batch Size**: Changed from 4 to 2 (more stable for object detection)\n",
    "- **Learning Rate**: Changed from 0.001 to 0.00025 (better for fine-tuning)\n",
    "\n",
    "### Expected Results:\n",
    "- ‚úì AP > 0 after epoch 1\n",
    "- ‚úì Training time ~1-2 hours/epoch (reasonable)\n",
    "- ‚úì Loss decreases AND AP increases together\n",
    "- ‚úì Model makes actual predictions during evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check PyTorch version for compatibility\n",
    "TORCH_VERSION = tuple(int(x) for x in torch.__version__.split('.')[:2])\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"TorchVision version: {torchvision.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    COCO Dataset with proper transforms and data augmentation\n",
    "    \"\"\"\n",
    "    def __init__(self, json_file, img_dir, transforms=None, train=False):\n",
    "        from pycocotools.coco import COCO\n",
    "        \n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.transforms = transforms\n",
    "        self.train = train\n",
    "        \n",
    "        # Load COCO annotations\n",
    "        print(f\"üìÇ Loading annotations from {json_file}...\")\n",
    "        try:\n",
    "            self.coco = COCO(str(json_file))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load COCO file: {e}\")\n",
    "            raise\n",
    "        \n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        \n",
    "        # Validate dataset\n",
    "        self._validate_dataset()\n",
    "        \n",
    "        print(f\"‚úì Loaded {len(self.ids)} images from {json_file}\")\n",
    "    \n",
    "    def _validate_dataset(self):\n",
    "        \"\"\"Validate dataset integrity\"\"\"\n",
    "        total_boxes = 0\n",
    "        images_without_boxes = 0\n",
    "        images_with_invalid_boxes = 0\n",
    "        \n",
    "        for img_id in self.ids:\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "            \n",
    "            if len(anns) == 0:\n",
    "                images_without_boxes += 1\n",
    "                continue\n",
    "            \n",
    "            # Check for invalid boxes\n",
    "            valid_boxes = 0\n",
    "            for ann in anns:\n",
    "                x, y, w, h = ann['bbox']\n",
    "                if w > 0 and h > 0:\n",
    "                    valid_boxes += 1\n",
    "            \n",
    "            if valid_boxes == 0:\n",
    "                images_with_invalid_boxes += 1\n",
    "            \n",
    "            total_boxes += valid_boxes\n",
    "        \n",
    "        if images_without_boxes > 0:\n",
    "            print(f\"   ‚ö†Ô∏è  {images_without_boxes} images have no annotations\")\n",
    "        \n",
    "        if images_with_invalid_boxes > 0:\n",
    "            print(f\"   ‚ö†Ô∏è  {images_with_invalid_boxes} images have only invalid boxes\")\n",
    "        \n",
    "        valid_images = len(self.ids) - images_without_boxes - images_with_invalid_boxes\n",
    "        \n",
    "        print(f\"   ‚úì Valid images: {valid_images}\")\n",
    "        print(f\"   ‚úì Total annotations: {total_boxes}\")\n",
    "        print(f\"   ‚úì Avg boxes/image: {total_boxes/len(self.ids):.2f}\")\n",
    "        \n",
    "        if valid_images == 0:\n",
    "            raise ValueError(\"No valid images found in dataset!\")\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        coco_anns = self.coco.loadAnns(ann_ids)\n",
    "        \n",
    "        # Load image\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = self.img_dir / img_info['file_name']\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Failed to load image {img_path}: {e}\")\n",
    "            # Return a dummy sample\n",
    "            img = Image.new('RGB', (640, 480))\n",
    "            coco_anns = []\n",
    "        \n",
    "        # Parse annotations\n",
    "        boxes, labels, areas = [], [], []\n",
    "        for ann in coco_anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            \n",
    "            # Skip invalid boxes\n",
    "            if w <= 0 or h <= 0:\n",
    "                continue\n",
    "            \n",
    "            # COCO bbox is [x, y, width, height]\n",
    "            # PyTorch needs [x1, y1, x2, y2]\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann['category_id'])\n",
    "            areas.append(ann['area'])\n",
    "        \n",
    "        # Apply data augmentation before converting to tensors\n",
    "        img_width, img_height = img.size\n",
    "        \n",
    "        # Random horizontal flip for training\n",
    "        if self.train and len(boxes) > 0 and np.random.rand() < 0.5:\n",
    "            # Flip image\n",
    "            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            \n",
    "            # Flip boxes: [x1, y1, x2, y2] -> [w-x2, y1, w-x1, y2]\n",
    "            flipped_boxes = []\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = box\n",
    "                flipped_boxes.append([img_width - x2, y1, img_width - x1, y2])\n",
    "            boxes = flipped_boxes\n",
    "        \n",
    "        # Convert to tensors\n",
    "        if len(boxes) > 0:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        else:\n",
    "            # Empty annotations\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros(0, dtype=torch.int64)\n",
    "            areas = torch.zeros(0, dtype=torch.float32)\n",
    "        \n",
    "        image_id = torch.tensor([img_id])\n",
    "        \n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": image_id,\n",
    "            \"area\": areas,\n",
    "            \"iscrowd\": torch.zeros(len(boxes), dtype=torch.int64)\n",
    "        }\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        \n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Function (FIXED - No ImageNet Normalization!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train=False):\n",
    "    \"\"\"\n",
    "    Create transforms for Faster R-CNN.\n",
    "    IMPORTANT: Faster R-CNN expects unnormalized images in [0, 255] range!\n",
    "    \n",
    "    Note: This function only handles image transforms. Data augmentation that\n",
    "    affects bounding boxes (like RandomHorizontalFlip) should be implemented\n",
    "    in the dataset's __getitem__ method to properly transform both images and boxes.\n",
    "    \"\"\"\n",
    "    transforms = []\n",
    "    \n",
    "    # Convert PIL to Tensor (scales to [0, 1])\n",
    "    transforms.append(torchvision.transforms.ToTensor())\n",
    "    \n",
    "    # Scale back to [0, 255] - Faster R-CNN expects this!\n",
    "    transforms.append(torchvision.transforms.Lambda(lambda x: x * 255.0))\n",
    "    \n",
    "    return torchvision.transforms.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes, anchor_sizes=None, pretrained=True):\n",
    "    \"\"\"\n",
    "    Create Faster R-CNN model with custom anchors\n",
    "    \n",
    "    Args:\n",
    "        num_classes: Number of classes (including background)\n",
    "        anchor_sizes: Tuple of anchor sizes, e.g., ((8,), (16,), (32,), (64,), (128,))\n",
    "        pretrained: Use pretrained backbone\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üèóÔ∏è  Creating Model\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load pretrained ResNet-50 FPN backbone\n",
    "    if pretrained:\n",
    "        try:\n",
    "            weights = torchvision.models.detection.FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "            model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=weights)\n",
    "            print(\"‚úì Loaded pretrained weights (FasterRCNN_ResNet50_FPN_V2)\")\n",
    "        except AttributeError:\n",
    "            # Fallback for older torchvision versions\n",
    "            weights = torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "            model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n",
    "            print(\"‚úì Loaded pretrained weights (FasterRCNN_ResNet50_FPN)\")\n",
    "    else:\n",
    "        try:\n",
    "            model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=None)\n",
    "        except AttributeError:\n",
    "            model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None)\n",
    "        print(\"‚úì Initialized model from scratch (NOT recommended)\")\n",
    "    \n",
    "    # Custom anchor generator if provided\n",
    "    if anchor_sizes is not None:\n",
    "        aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "        anchor_generator = AnchorGenerator(\n",
    "            sizes=anchor_sizes,\n",
    "            aspect_ratios=aspect_ratios\n",
    "        )\n",
    "        model.rpn.anchor_generator = anchor_generator\n",
    "        print(f\"‚úì Custom RPN anchors: {anchor_sizes}\")\n",
    "        print(f\"  Aspect ratios: {aspect_ratios[0]}\")\n",
    "    else:\n",
    "        print(f\"‚úì Using default anchors\")\n",
    "    \n",
    "    # Replace box predictor head for our number of classes\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    print(f\"‚úì Replaced box predictor for {num_classes} classes\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for batching\n",
    "    Handles variable-sized images and annotations\n",
    "    \"\"\"\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, scaler=None):\n",
    "    \"\"\"\n",
    "    Train for one epoch with proper error handling\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    loss_dict_cumulative = {}\n",
    "    num_batches = 0\n",
    "    num_skipped = 0\n",
    "    \n",
    "    pbar = tqdm(data_loader, desc=f\"Epoch {epoch} [Train]\")\n",
    "    \n",
    "    for images, targets in pbar:\n",
    "        try:\n",
    "            # Move to device\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Filter out images with no boxes (CRITICAL FIX)\n",
    "            valid_idx = [i for i, t in enumerate(targets) if len(t['boxes']) > 0]\n",
    "            if len(valid_idx) == 0:\n",
    "                num_skipped += 1\n",
    "                continue\n",
    "            \n",
    "            images = [images[i] for i in valid_idx]\n",
    "            targets = [targets[i] for i in valid_idx]\n",
    "            \n",
    "            # Forward pass\n",
    "            if scaler is not None:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss_dict = model(images, targets)\n",
    "                    losses = sum(loss for loss in loss_dict.values())\n",
    "                \n",
    "                # Check for NaN\n",
    "                if torch.isnan(losses) or torch.isinf(losses):\n",
    "                    print(f\"\\n‚ö†Ô∏è  NaN/Inf loss detected, skipping batch\")\n",
    "                    num_skipped += 1\n",
    "                    continue\n",
    "                \n",
    "                # Backward pass with gradient scaling\n",
    "                optimizer.zero_grad()\n",
    "                scaler.scale(losses).backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "                \n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                \n",
    "                # Check for NaN\n",
    "                if torch.isnan(losses) or torch.isinf(losses):\n",
    "                    print(f\"\\n‚ö†Ô∏è  NaN/Inf loss detected, skipping batch\")\n",
    "                    num_skipped += 1\n",
    "                    continue\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                losses.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "            \n",
    "            # Accumulate losses (detach from graph)\n",
    "            total_loss += losses.item()\n",
    "            for k, v in loss_dict.items():\n",
    "                if k not in loss_dict_cumulative:\n",
    "                    loss_dict_cumulative[k] = 0.0\n",
    "                loss_dict_cumulative[k] += v.item()\n",
    "            \n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{losses.item():.4f}\",\n",
    "                'avg': f\"{total_loss/num_batches:.4f}\",\n",
    "                'skip': num_skipped\n",
    "            })\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  Runtime error in batch: {e}\")\n",
    "            num_skipped += 1\n",
    "            continue\n",
    "    \n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "    \n",
    "    # Print detailed loss breakdown\n",
    "    if num_batches > 0:\n",
    "        print(f\"\\n  Loss breakdown:\")\n",
    "        for k, v in loss_dict_cumulative.items():\n",
    "            print(f\"    {k}: {v/num_batches:.4f}\")\n",
    "    \n",
    "    if num_skipped > 0:\n",
    "        print(f\"  ‚ö†Ô∏è  Skipped {num_skipped} batches due to errors\")\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device, coco_gt):\n",
    "    \"\"\"\n",
    "    Evaluate using official COCO metrics with error handling\n",
    "    \"\"\"\n",
    "    from pycocotools.cocoeval import COCOeval\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    coco_results = []\n",
    "    num_errors = 0\n",
    "    \n",
    "    pbar = tqdm(data_loader, desc=\"Evaluating\")\n",
    "    \n",
    "    for images, targets in pbar:\n",
    "        try:\n",
    "            images = [img.to(device) for img in images]\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Convert to COCO format\n",
    "            for target, output in zip(targets, outputs):\n",
    "                image_id = target['image_id'].item()\n",
    "                \n",
    "                boxes = output['boxes'].cpu().numpy()\n",
    "                scores = output['scores'].cpu().numpy()\n",
    "                labels = output['labels'].cpu().numpy()\n",
    "                \n",
    "                # Filter by score threshold\n",
    "                score_threshold = 0.05  # Low threshold for evaluation\n",
    "                mask = scores > score_threshold\n",
    "                \n",
    "                boxes = boxes[mask]\n",
    "                scores = scores[mask]\n",
    "                labels = labels[mask]\n",
    "                \n",
    "                for box, score, label in zip(boxes, scores, labels):\n",
    "                    # Convert [x1, y1, x2, y2] to [x, y, w, h]\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    w = x2 - x1\n",
    "                    h = y2 - y1\n",
    "                    \n",
    "                    # Skip invalid predictions\n",
    "                    if w <= 0 or h <= 0:\n",
    "                        continue\n",
    "                    \n",
    "                    coco_results.append({\n",
    "                        'image_id': int(image_id),\n",
    "                        'category_id': int(label),\n",
    "                        'bbox': [float(x1), float(y1), float(w), float(h)],\n",
    "                        'score': float(score)\n",
    "                    })\n",
    "        \n",
    "        except Exception as e:\n",
    "            num_errors += 1\n",
    "            pbar.set_postfix({'errors': num_errors})\n",
    "            continue\n",
    "    \n",
    "    # Evaluate with COCO API\n",
    "    if len(coco_results) == 0:\n",
    "        print(\"\\n‚ö†Ô∏è  No predictions made!\")\n",
    "        return 0.0, 0.0, 0.0\n",
    "    \n",
    "    try:\n",
    "        coco_dt = coco_gt.loadRes(coco_results)\n",
    "        coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "        coco_eval.evaluate()\n",
    "        coco_eval.accumulate()\n",
    "        coco_eval.summarize()\n",
    "        \n",
    "        # Extract key metrics\n",
    "        map_50_95 = coco_eval.stats[0]  # AP @ IoU=0.50:0.95\n",
    "        ap50 = coco_eval.stats[1]       # AP @ IoU=0.50\n",
    "        ap75 = coco_eval.stats[2]       # AP @ IoU=0.75\n",
    "        \n",
    "        return map_50_95, ap50, ap75\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è  COCO evaluation failed: {e}\")\n",
    "        print(f\"    Generated {len(coco_results)} predictions\")\n",
    "        return 0.0, 0.0, 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration with FIXED hyperparameters\n",
    "config = {\n",
    "    # Data\n",
    "    'data_dir': './data',\n",
    "    'output_dir': './runs/rcnn_v3',\n",
    "    \n",
    "    # Model\n",
    "    'num_classes': 2,  # Background + car\n",
    "    'pretrained': True,\n",
    "    'anchor_sizes': None,  # Use default anchors\n",
    "    \n",
    "    # Training - FIXED VALUES\n",
    "    'epochs': 50,\n",
    "    'batch_size': 2,      # Changed from 4/6 to 2 for stability\n",
    "    'workers': 4,\n",
    "    \n",
    "    # Optimizer - FIXED VALUES\n",
    "    'optimizer': 'sgd',\n",
    "    'lr': 0.00025,        # Changed from 0.001 to 0.00025 for fine-tuning\n",
    "    'weight_decay': 0.0005,\n",
    "    'momentum': 0.9,\n",
    "    \n",
    "    # Scheduler\n",
    "    'scheduler': 'step',\n",
    "    'lr_step_size': 15,\n",
    "    'lr_gamma': 0.1,\n",
    "    \n",
    "    # Other\n",
    "    'patience': 15,\n",
    "    'amp': True,  # Use mixed precision if available\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Device and Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nüñ•Ô∏è  Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(config['output_dir'])\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"\\nüìÅ Output directory: {output_dir.absolute()}\")\n",
    "\n",
    "# Save configuration\n",
    "with open(output_dir / 'config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f\"‚úì Configuration saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìö Loading Datasets\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "data_path = Path(config['data_dir'])\n",
    "\n",
    "# Verify data exists\n",
    "if not (data_path / 'train.json').exists():\n",
    "    raise FileNotFoundError(f\"Training data not found at {data_path / 'train.json'}\")\n",
    "\n",
    "if not (data_path / 'val.json').exists():\n",
    "    raise FileNotFoundError(f\"Validation data not found at {data_path / 'val.json'}\")\n",
    "\n",
    "# Create datasets with FIXED transforms (no ImageNet normalization!)\n",
    "train_dataset = COCODataset(\n",
    "    json_file=data_path / 'train.json',\n",
    "    img_dir=data_path / 'train',\n",
    "    transforms=get_transform(train=True),\n",
    "    train=True  # Enable augmentation\n",
    ")\n",
    "\n",
    "val_dataset = COCODataset(\n",
    "    json_file=data_path / 'val.json',\n",
    "    img_dir=data_path / 'val',\n",
    "    transforms=get_transform(train=False),\n",
    "    train=False  # No augmentation for validation\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Train dataset: {len(train_dataset)} images\")\n",
    "print(f\"‚úì Val dataset: {len(val_dataset)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "use_persistent_workers = config['workers'] > 0 and TORCH_VERSION >= (1, 7)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=config['workers'],\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    persistent_workers=use_persistent_workers\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=config['workers'],\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    persistent_workers=use_persistent_workers\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì DataLoaders created:\")\n",
    "print(f\"  Workers: {config['workers']}\")\n",
    "print(f\"  Persistent workers: {use_persistent_workers}\")\n",
    "print(f\"  Batch size: {config['batch_size']}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(\n",
    "    num_classes=config['num_classes'],\n",
    "    anchor_sizes=config['anchor_sizes'],\n",
    "    pretrained=config['pretrained']\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nüìä Model Parameters:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Trainable: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer with FIXED learning rate\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "if config['optimizer'] == 'sgd':\n",
    "    optimizer = torch.optim.SGD(\n",
    "        params,\n",
    "        lr=config['lr'],\n",
    "        momentum=config['momentum'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    print(f\"\\n‚úì Using SGD optimizer\")\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params,\n",
    "        lr=config['lr'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    print(f\"\\n‚úì Using AdamW optimizer\")\n",
    "\n",
    "print(f\"  Learning rate: {config['lr']} (FIXED from 0.001)\")\n",
    "print(f\"  Weight decay: {config['weight_decay']}\")\n",
    "\n",
    "# Learning rate scheduler\n",
    "if config['scheduler'] == 'step':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=config['lr_step_size'],\n",
    "        gamma=config['lr_gamma']\n",
    "    )\n",
    "    print(f\"‚úì Using StepLR scheduler\")\n",
    "    print(f\"  Step size: {config['lr_step_size']}\")\n",
    "    print(f\"  Gamma: {config['lr_gamma']}\")\n",
    "elif config['scheduler'] == 'cosine':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=config['epochs'],\n",
    "        eta_min=1e-6\n",
    "    )\n",
    "    print(f\"‚úì Using CosineAnnealingLR scheduler\")\n",
    "else:  # multistep\n",
    "    milestones = [int(config['epochs'] * 0.6), int(config['epochs'] * 0.8)]\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer,\n",
    "        milestones=milestones,\n",
    "        gamma=0.1\n",
    "    )\n",
    "    print(f\"‚úì Using MultiStepLR scheduler\")\n",
    "    print(f\"  Milestones: {milestones}\")\n",
    "\n",
    "# Mixed precision training\n",
    "scaler = None\n",
    "if config['amp'] and torch.cuda.is_available():\n",
    "    if hasattr(torch.cuda.amp, 'GradScaler'):\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        print(\"‚úì Using mixed precision training (AMP)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  AMP not available in this PyTorch version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üèãÔ∏è  Training\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_map = 0.0\n",
    "best_ap50 = 0.0\n",
    "best_ap75 = 0.0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_map': [],\n",
    "    'val_ap50': [],\n",
    "    'val_ap75': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "try:\n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Epoch {epoch}/{config['epochs']}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_one_epoch(\n",
    "            model, optimizer, train_loader, device, epoch, scaler\n",
    "        )\n",
    "        \n",
    "        # Update learning rate\n",
    "        lr_scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Evaluate\n",
    "        print(f\"\\nüìä Evaluating Epoch {epoch}\")\n",
    "        val_map, ap50, ap75 = evaluate(model, val_loader, device, val_dataset.coco)\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(float(train_loss))\n",
    "        history['val_map'].append(float(val_map))\n",
    "        history['val_ap50'].append(float(ap50))\n",
    "        history['val_ap75'].append(float(ap75))\n",
    "        history['lr'].append(float(current_lr))\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Epoch {epoch} Summary:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val mAP@[.5:.95]: {val_map:.4f}\")\n",
    "        print(f\"  Val AP@0.50: {ap50:.4f}\")\n",
    "        print(f\"  Val AP@0.75: {ap75:.4f}\")\n",
    "        print(f\"  Learning Rate: {current_lr:.6f}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Save latest checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "            'metrics': {\n",
    "                'train_loss': train_loss,\n",
    "                'val_map': val_map,\n",
    "                'val_ap50': ap50,\n",
    "                'val_ap75': ap75,\n",
    "                'history': history\n",
    "            },\n",
    "            'torch_version': torch.__version__,\n",
    "            'torchvision_version': torchvision.__version__,\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, output_dir / 'latest.pt')\n",
    "        \n",
    "        # Save best checkpoint\n",
    "        if ap50 > best_ap50:\n",
    "            best_map = val_map\n",
    "            best_ap50 = ap50\n",
    "            best_ap75 = ap75\n",
    "            epochs_no_improve = 0\n",
    "            \n",
    "            torch.save(checkpoint, output_dir / 'best.pt')\n",
    "            print(f\"üéâ New best model saved!\")\n",
    "            print(f\"   mAP@[.5:.95]: {best_map:.4f}\")\n",
    "            print(f\"   AP@0.50: {best_ap50:.4f}\")\n",
    "            print(f\"   AP@0.75: {best_ap75:.4f}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement for {epochs_no_improve} epoch(s)\")\n",
    "            print(f\"Best AP@0.50: {best_ap50:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if epochs_no_improve >= config['patience']:\n",
    "            print(f\"\\n‚èπÔ∏è  Early stopping triggered after {config['patience']} epochs without improvement\")\n",
    "            break\n",
    "        \n",
    "        # Save periodic checkpoint\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save(checkpoint, output_dir / f'checkpoint_epoch_{epoch}.pt')\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è  Training interrupted by user\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    # Save final history\n",
    "    with open(output_dir / 'history.json', 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ Training Complete!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Best Results:\")\n",
    "    print(f\"  mAP@[.5:.95]: {best_map:.4f}\")\n",
    "    print(f\"  AP@0.50: {best_ap50:.4f}\")\n",
    "    print(f\"  AP@0.75: {best_ap75:.4f}\")\n",
    "    print(f\"\\nModels saved to: {output_dir.absolute()}\")\n",
    "    print(f\"  - best.pt (best AP@0.50)\")\n",
    "    print(f\"  - latest.pt (most recent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training History (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Plot training curves\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history['train_loss'])\n",
    "    axes[0, 0].set_title('Training Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # mAP\n",
    "    axes[0, 1].plot(history['val_map'], label='mAP@[.5:.95]')\n",
    "    axes[0, 1].plot(history['val_ap50'], label='AP@0.50')\n",
    "    axes[0, 1].plot(history['val_ap75'], label='AP@0.75')\n",
    "    axes[0, 1].set_title('Validation Metrics')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('AP')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Learning Rate\n",
    "    axes[1, 0].plot(history['lr'])\n",
    "    axes[1, 0].set_title('Learning Rate')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('LR')\n",
    "    axes[1, 0].set_yscale('log')\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Loss vs AP\n",
    "    axes[1, 1].plot(history['train_loss'], label='Loss (‚Üì)')\n",
    "    axes[1, 1].plot(history['val_ap50'], label='AP@0.50 (‚Üë)')\n",
    "    axes[1, 1].set_title('Loss vs AP@0.50')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úì Training curves saved to {output_dir / 'training_curves.png'}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  matplotlib not available, skipping plots\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
