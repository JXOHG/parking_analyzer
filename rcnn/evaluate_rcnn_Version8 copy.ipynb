{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNN Model Evaluation and Visualization\n",
    "\n",
    "This notebook evaluates the Faster R-CNN model trained by `train_rcnn.py`.\n",
    "\n",
    "**Key Features:**\n",
    "1.  **Load Latest Model**: Loads the `latest.pt` checkpoint from a training run.\n",
    "2.  **Evaluate on Test Set**: Computes COCO metrics (mAP, AP50, AP75) on the unseen test set.\n",
    "3.  **Calculate Performance Metrics**: Calculates inference speed (FPS), Car Count Accuracy, and Mean Absolute Error (MAE) for model comparison.\n",
    "4.  **Visualize Predictions**: Shows a side-by-side comparison of ground truth and model predictions on test images.\n",
    "\n",
    "<b>Note: Training script could not be run on jupyter notebook due to multithreading issues! Refer to train_rcnn.py</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "This section imports necessary libraries and defines the paths to the model, data, and training artifacts. Please ensure the paths below are configured correctly for your project structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration ---\n",
    "RUN_DIR = Path('./runs/rcnn/')  # Directory where 'latest.pt' is saved\n",
    "DATA_DIR = Path('./data/')      # Directory containing the 'test' folder and 'test.json'\n",
    "MODEL_CHECKPOINT = RUN_DIR / 'latest.pt'\n",
    "TEST_JSON = DATA_DIR / 'test.json'\n",
    "TEST_IMG_DIR = DATA_DIR / 'test'\n",
    "\n",
    "# --- Evaluation Parameters ---\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_CLASSES = 2  # 1 (car) + 1 (background)\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 0  # Set to 0 to prevent multiprocessing issues in Jupyter. This was a consistent issue I did not know how to fix...\n",
    "SCORE_THRESHOLD = 0.5 # Threshold for counting predictions and visualization\n",
    "\n",
    "# Check that paths exist\n",
    "if not MODEL_CHECKPOINT.exists():\n",
    "    raise FileNotFoundError(f\"Model checkpoint not found at: {MODEL_CHECKPOINT}\")\n",
    "if not TEST_JSON.exists():\n",
    "    raise FileNotFoundError(f\"Test annotations not found at: {TEST_JSON}\")\n",
    "if not TEST_IMG_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Test images not found at: {TEST_IMG_DIR}\")\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Using model: {MODEL_CHECKPOINT}\")\n",
    "print(f\"Using test data: {TEST_JSON}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions and Classes\n",
    "\n",
    "The helper functions and classes from the training script are included here to make the notebook self-contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition function from the training script\n",
    "def get_model(num_classes, anchor_sizes=None, pretrained=False):\n",
    "    # Note: pretrained=False because we are loading our own trained weights.\n",
    "    # Use the V2 model if available, otherwise fallback\n",
    "    try:\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=None)\n",
    "        print(\"Using FasterRCNN_ResNet50_FPN_V2 architecture.\")\n",
    "    except AttributeError:\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None)\n",
    "        print(\"Using FasterRCNN_ResNet50_FPN architecture.\")\n",
    "    \n",
    "    if anchor_sizes is not None:\n",
    "        aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "        anchor_generator = AnchorGenerator(sizes=anchor_sizes, aspect_ratios=aspect_ratios)\n",
    "        model.rpn.anchor_generator = anchor_generator\n",
    "    \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# COCODataset class from the training script\n",
    "class COCODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, json_file, img_dir, transforms=None):\n",
    "        from pycocotools.coco import COCO\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.transforms = transforms\n",
    "        self.coco = COCO(str(json_file))\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        coco_anns = self.coco.loadAnns(ann_ids)\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = self.img_dir / img_info['file_name']\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        boxes, labels = [], []\n",
    "        for ann in coco_anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            if w <= 0 or h <= 0: continue\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann['category_id'])\n",
    "        \n",
    "        target = {\n",
    "            \"boxes\": torch.as_tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4), dtype=torch.float32),\n",
    "            \"labels\": torch.as_tensor(labels, dtype=torch.int64) if labels else torch.zeros(0, dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([img_id])\n",
    "        }\n",
    "        \n",
    "        if self.transforms:\n",
    "            img_tensor = self.transforms(img)\n",
    "        \n",
    "        return img_tensor, target, img # Return original PIL image for visualization\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "# Transforms and collate function\n",
    "def get_transform():\n",
    "    return torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training arguments to check for custom anchor sizes\n",
    "args_file = RUN_DIR / 'args.json'\n",
    "anchor_sizes = None\n",
    "if args_file.exists():\n",
    "    with open(args_file, 'r') as f:\n",
    "        train_args = json.load(f)\n",
    "    if train_args.get('anchor_sizes'):\n",
    "        anchor_sizes = tuple(tuple([int(s)]) for s in train_args['anchor_sizes'].split(','))\n",
    "        print(f\"Using custom anchor sizes from training: {anchor_sizes}\")\n",
    "else:\n",
    "    print(\"Training args.json not found, using default model parameters.\")\n",
    "\n",
    "# Load model\n",
    "model = get_model(num_classes=NUM_CLASSES, anchor_sizes=anchor_sizes)\n",
    "checkpoint = torch.load(MODEL_CHECKPOINT, map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Create test dataset and dataloader\n",
    "test_dataset = COCODataset(TEST_JSON, TEST_IMG_DIR, transforms=get_transform())\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "print(f\"\\nTest dataset contains {len(test_dataset)} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Model on Test Set (COCO Metrics)\n",
    "\n",
    "This function runs the model on the test set, computes the standard COCO evaluation metrics, and returns the detailed stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device, coco_gt):\n",
    "    from pycocotools.cocoeval import COCOeval\n",
    "    \n",
    "    model.eval()\n",
    "    coco_results = []\n",
    "    \n",
    "    for images, targets, _ in tqdm(data_loader, desc=\"Generating Predictions\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        outputs = model(images)\n",
    "        \n",
    "        for target, output in zip(targets, outputs):\n",
    "            image_id = target['image_id'].item()\n",
    "            boxes = output['boxes'].cpu().numpy()\n",
    "            scores = output['scores'].cpu().numpy()\n",
    "            labels = output['labels'].cpu().numpy()\n",
    "            \n",
    "            mask = scores > 0.05\n",
    "            boxes, scores, labels = boxes[mask], scores[mask], labels[mask]\n",
    "            \n",
    "            for box, score, label in zip(boxes, scores, labels):\n",
    "                if label == 0: continue\n",
    "                x1, y1, x2, y2 = box\n",
    "                w, h = x2 - x1, y2 - y1\n",
    "                if w <= 0 or h <= 0: continue\n",
    "                \n",
    "                coco_results.append({\n",
    "                    'image_id': int(image_id),\n",
    "                    'category_id': 1,\n",
    "                    'bbox': [float(x1), float(y1), float(w), float(h)],\n",
    "                    'score': float(score)\n",
    "                })\n",
    "                \n",
    "    if not coco_results:\n",
    "        print(\"\\nNo predictions were made. Cannot evaluate.\")\n",
    "        return None\n",
    "        \n",
    "    print(f\"\\nGenerated {len(coco_results)} predictions.\")\n",
    "\n",
    "    if 'info' not in coco_gt.dataset:\n",
    "        coco_gt.dataset['info'] = []\n",
    "        \n",
    "    coco_dt = coco_gt.loadRes(coco_results)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "    \n",
    "    return coco_eval.stats\n",
    "\n",
    "# Run COCO evaluation and store the stats\n",
    "coco_stats = evaluate(model, test_loader, DEVICE, test_dataset.coco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate Key Performance Metrics\n",
    "\n",
    "This section calculates the inference speed (FPS) and counting performance, organizing the most important metrics into a clear summary for model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def calculate_fps(model, data_loader, device):\n",
    "    model.eval()\n",
    "    print(\"Performing warm-up runs for FPS calculation...\")\n",
    "    for i, (images, _, _) in enumerate(data_loader):\n",
    "        if i >= 3: break\n",
    "        images = [img.to(device) for img in images]\n",
    "        _ = model(images)\n",
    "\n",
    "    print(\"Calculating FPS...\")\n",
    "    total_time, num_images = 0, 0\n",
    "    for images, _, _ in tqdm(data_loader, desc=\"Measuring Inference Speed\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        start_time = time.time()\n",
    "        _ = model(images)\n",
    "        end_time = time.time()\n",
    "        total_time += (end_time - start_time)\n",
    "        num_images += len(images)\n",
    "    return num_images / total_time if total_time > 0 else 0\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_counting_metrics(model, data_loader, device, score_threshold):\n",
    "    model.eval()\n",
    "    correct_counts, total_images, total_absolute_error = 0, 0, 0\n",
    "    for images, targets, _ in tqdm(data_loader, desc=\"Calculating Counting Metrics\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        outputs = model(images)\n",
    "        for i in range(len(images)):\n",
    "            gt_count = len(targets[i]['labels'])\n",
    "            pred_scores = outputs[i]['scores']\n",
    "            pred_count = (pred_scores > score_threshold).sum().item()\n",
    "            \n",
    "            total_absolute_error += abs(gt_count - pred_count)\n",
    "            if gt_count == pred_count:\n",
    "                correct_counts += 1\n",
    "            total_images += 1\n",
    "            \n",
    "    exact_accuracy = (correct_counts / total_images) * 100 if total_images > 0 else 0\n",
    "    mae = total_absolute_error / total_images if total_images > 0 else 0\n",
    "    return exact_accuracy, mae\n",
    "\n",
    "# Calculate metrics\n",
    "fps = calculate_fps(model, test_loader, DEVICE)\n",
    "count_accuracy, count_mae = calculate_counting_metrics(model, test_loader, DEVICE, SCORE_THRESHOLD)\n",
    "\n",
    "# --- Display Summary --- #\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"            Model Performance Summary\")\n",
    "print(\"=\"*50)\n",
    "if coco_stats is not None:\n",
    "    print(f\" Bbox AP @ IoU=.50:.95:              {coco_stats[0]:.4f}\")\n",
    "    print(f\" Bbox AP @ IoU=.50:                   {coco_stats[1]:.4f}\")\n",
    "    print(f\" Bbox AR @ maxDets=100:             {coco_stats[8]:.4f}\")\n",
    "else:\n",
    "    print(\" Bounding box metrics not available.\")\n",
    "print(\"-\"*50)\n",
    "print(f\" Car Count Exact Accuracy:            {count_accuracy:.2f}%\")\n",
    "print(f\" Car Count Mean Absolute Error (MAE): {count_mae:.4f}\")\n",
    "print(\"-\"*50)\n",
    "print(f\" Inference Speed (FPS):               {fps:.2f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Predictions on Test Images\n",
    "\n",
    "Finally, let's see the model in action. We'll sample a few images from the test set and draw both the ground truth bounding boxes and the model's predicted boxes. This provides a qualitative assessment of the model's performance.\n",
    "\n",
    "-   **Green Boxes**: Ground Truth\n",
    "-   **Red Boxes**: Model Predictions (with confidence scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image, boxes, labels, color, text_prefix=''):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 15)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    for box, label in zip(boxes, labels):\n",
    "        draw.rectangle(list(box), outline=color, width=3)\n",
    "        text = f\"{text_prefix}{label}\"\n",
    "        text_size = draw.textbbox((0,0), text, font=font)\n",
    "        text_width = text_size[2] - text_size[0]\n",
    "        text_height = text_size[3] - text_size[1]\n",
    "        draw.rectangle([box[0], box[1] - text_height, box[0] + text_width, box[1]], fill=color)\n",
    "        draw.text((box[0], box[1] - text_height), text, fill=\"white\", font=font)\n",
    "\n",
    "def visualize_predictions(model, dataset, num_samples=5, score_threshold=0.5):\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        img_tensor, target, original_img = dataset[idx]\n",
    "        \n",
    "        # --- Ground Truth ---\n",
    "        gt_img = original_img.copy()\n",
    "        gt_boxes = target['boxes'].numpy()\n",
    "        gt_count = len(gt_boxes)\n",
    "        gt_labels = [f\"car (Total: {gt_count})\"] * gt_count if gt_count > 0 else []\n",
    "        draw_boxes(gt_img, gt_boxes, gt_labels, color='green')\n",
    "        \n",
    "        # --- Prediction ---\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            prediction = model([img_tensor.to(DEVICE)])[0]\n",
    "        \n",
    "        pred_img = original_img.copy()\n",
    "        boxes = prediction['boxes'].cpu().numpy()\n",
    "        scores = prediction['scores'].cpu().numpy()\n",
    "        \n",
    "        mask = scores > score_threshold\n",
    "        boxes, scores = boxes[mask], scores[mask]\n",
    "        pred_count = len(scores)\n",
    "        \n",
    "        pred_labels = [f\"car: {s:.2f}\" for s in scores]\n",
    "        if pred_labels:\n",
    "             pred_labels[0] = f\"car: {scores[0]:.2f} (Total: {pred_count})\"\n",
    "        draw_boxes(pred_img, boxes, pred_labels, color='red')\n",
    "        \n",
    "        # --- Display ---\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        ax1.imshow(gt_img)\n",
    "        ax1.set_title(f'Ground Truth (Count: {gt_count})')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        ax2.imshow(pred_img)\n",
    "        ax2.set_title(f'Model Prediction (Count: {pred_count})')\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "# Visualize some predictions\n",
    "visualize_predictions(model, test_dataset, num_samples=5, score_threshold=SCORE_THRESHOLD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
