{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison: YOLOv9 vs Faster R-CNN\n",
    "\n",
    "This notebook provides a direct comparison between the trained YOLOv9 and Faster R-CNN models on the parking lot detection task.\n",
    "\n",
    "**Key Comparisons:**\n",
    "1. **R-CNN Learning Curves:** Dedicated visualization matching YOLO style.\n",
    "2. **Visual Comparison:** 3-column view (Ground Truth vs YOLO vs R-CNN).\n",
    "3. **Counting Error Analysis:** Single graph showing Ground Truth vs Prediction Error for both models.\n",
    "4. **Statistical Summary:** Count error (Mean ± Std Dev) and inference speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "YOLO Weights: yolo\\runs\\train\\carpk_yolov9\\weights\\best.pt\n",
      "RCNN Weights: rcnn\\runs\\rcnn\\latest.pt\n",
      "RCNN History: rcnn\\runs\\runs\\rcnn\\history.json\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import json\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- PATH CONFIGURATION ---\n",
    "# Root directories\n",
    "YOLO_BASE = Path('./yolo')\n",
    "RCNN_BASE = Path('./rcnn')\n",
    "\n",
    "# YOLO Paths\n",
    "YOLO_ROOT = YOLO_BASE / 'yolov9'\n",
    "YOLO_WEIGHTS = YOLO_BASE / 'runs/train/carpk_yolov9/weights/best.pt'\n",
    "YOLO_DATA_YAML = YOLO_BASE / 'prepared_data/yolo/data.yaml'\n",
    "\n",
    "# RCNN Paths\n",
    "RCNN_WEIGHTS = RCNN_BASE / 'runs/rcnn/latest.pt'\n",
    "RCNN_HISTORY = RCNN_BASE / 'runs/runs/rcnn/history.json'\n",
    "RCNN_TEST_JSON = RCNN_BASE / 'data/test.json'\n",
    "RCNN_TEST_IMG_DIR = RCNN_BASE / 'data/test'\n",
    "\n",
    "# Common Config\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "IMG_SIZE = 640\n",
    "CONF_THRESHOLD = 0.25  # Confidence threshold for detection\n",
    "IOU_THRESHOLD = 0.45   # NMS IoU threshold\n",
    "\n",
    "# Check files\n",
    "required_files = [YOLO_ROOT, YOLO_WEIGHTS, YOLO_DATA_YAML, RCNN_WEIGHTS, RCNN_HISTORY, RCNN_TEST_JSON]\n",
    "for p in required_files:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing required file/directory: {p}\")\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"YOLO Weights: {YOLO_WEIGHTS}\")\n",
    "print(f\"RCNN Weights: {RCNN_WEIGHTS}\")\n",
    "print(f\"RCNN History: {RCNN_HISTORY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Models\n",
    "Standardized wrappers for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLOv9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "yolov9-c summary: 604 layers, 50698278 parameters, 0 gradients, 236.6 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Faster R-CNN...\n",
      "Loading R-CNN weights from rcnn\\runs\\rcnn\\latest.pt\n",
      "Models loaded.\n"
     ]
    }
   ],
   "source": [
    "class YOLOWrapper:\n",
    "    def __init__(self, weights_path, repo_path, device=DEVICE):\n",
    "        self.device = device\n",
    "        repo_str = str(repo_path.absolute())\n",
    "        if repo_str not in sys.path:\n",
    "            sys.path.append(repo_str)\n",
    "            \n",
    "        try:\n",
    "            from models.common import DetectMultiBackend\n",
    "            from utils.general import non_max_suppression, scale_boxes\n",
    "            from utils.augmentations import letterbox\n",
    "        except ImportError as e:\n",
    "            print(f\"Error importing YOLO modules from {repo_path}: {e}\")\n",
    "            raise\n",
    "        \n",
    "        self.model = DetectMultiBackend(weights_path, device=device, fp16=False)\n",
    "        self.stride = self.model.stride\n",
    "        self.names = self.model.names\n",
    "        self.pt = self.model.pt\n",
    "        self.non_max_suppression = non_max_suppression\n",
    "        self.scale_boxes = scale_boxes\n",
    "        self.letterbox = letterbox\n",
    "\n",
    "    def predict(self, img_pil, conf_thres=0.25, iou_thres=0.45):\n",
    "        im0 = np.array(img_pil)\n",
    "        im = self.letterbox(im0, 640, stride=self.stride, auto=True)[0]\n",
    "        im = im.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
    "        im = np.ascontiguousarray(im)\n",
    "        im = torch.from_numpy(im).to(self.device)\n",
    "        im = im.float() / 255.0\n",
    "        if len(im.shape) == 3:\n",
    "            im = im[None]\n",
    "\n",
    "        pred = self.model(im, augment=False, visualize=False)\n",
    "        if isinstance(pred, list):\n",
    "            pred = pred[0]\n",
    "\n",
    "        pred = self.non_max_suppression(pred, conf_thres, iou_thres, classes=None, max_det=1000)\n",
    "        results = []\n",
    "        det = pred[0]\n",
    "        if len(det):\n",
    "            det[:, :4] = self.scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                results.append({\n",
    "                    'box': [int(c) for c in xyxy],\n",
    "                    'score': float(conf),\n",
    "                    'label': 'car'\n",
    "                })\n",
    "        return results\n",
    "\n",
    "class RCNNWrapper:\n",
    "    def __init__(self, weights_path, device=DEVICE):\n",
    "        self.device = device\n",
    "        def get_rcnn_model(num_classes):\n",
    "            try:\n",
    "                model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=None)\n",
    "            except AttributeError:\n",
    "                model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None)\n",
    "            in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "            model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "            return model\n",
    "\n",
    "        self.model = get_rcnn_model(num_classes=2)\n",
    "        print(f\"Loading R-CNN weights from {weights_path}\")\n",
    "        checkpoint = torch.load(weights_path, map_location=device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.transforms = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def predict(self, img_pil, conf_thres=0.25):\n",
    "        img_tensor = self.transforms(img_pil).to(self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            pred = self.model(img_tensor)[0]\n",
    "        \n",
    "        results = []\n",
    "        boxes = pred['boxes'].cpu().numpy()\n",
    "        scores = pred['scores'].cpu().numpy()\n",
    "        labels = pred['labels'].cpu().numpy()\n",
    "        \n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            if score >= conf_thres and label == 1: # 1 is car\n",
    "                results.append({\n",
    "                    'box': [int(c) for c in box],\n",
    "                    'score': float(score),\n",
    "                    'label': 'car'\n",
    "                })\n",
    "        return results\n",
    "\n",
    "print(\"Loading YOLOv9...\")\n",
    "yolo_model = YOLOWrapper(YOLO_WEIGHTS, YOLO_ROOT)\n",
    "print(\"Loading Faster R-CNN...\")\n",
    "rcnn_model = RCNNWrapper(RCNN_WEIGHTS)\n",
    "print(\"Models loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Faster R-CNN Training Learning Curves\n",
    "Visualizing training metrics for Faster R-CNN in the same style as YOLOv9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rcnn_learning_curves():\n",
    "    with open(RCNN_HISTORY, 'r') as f:\n",
    "        h = json.load(f)\n",
    "        \n",
    "    epochs = range(1, len(h['train_loss']) + 1)\n",
    "    total_epochs = len(epochs)\n",
    "    \n",
    "    # Create figure with grid spec (2x2)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    fig.suptitle('Faster R-CNN Training Learning Curves - Parking Dataset', fontsize=22, fontweight='bold', y=0.95)\n",
    "    \n",
    "    # --- 1. Training Loss Components ---\n",
    "    ax = axes[0, 0]\n",
    "    # Colors matching warm tones for training\n",
    "    ax.plot(epochs, h.get('loss_classifier', []), color='#D32F2F', linewidth=2, label='Train Cls Loss')\n",
    "    ax.plot(epochs, h.get('loss_box_reg', []), color='#F57C00', linewidth=2, label='Train Box Loss')\n",
    "    ax.plot(epochs, h.get('loss_objectness', []), color='#FFB300', linewidth=2, label='Train Obj Loss')\n",
    "    ax.plot(epochs, h.get('loss_rpn_box_reg', []), color='#FBC02D', linewidth=2, linestyle='--', label='Train RPN Box Loss')\n",
    "    \n",
    "    ax.set_title('Training Loss Components', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch', fontsize=14)\n",
    "    ax.set_ylabel('Loss', fontsize=14)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='upper right', frameon=True, fontsize=11)\n",
    "    \n",
    "    # --- 2. Mean Average Precision (mAP) ---\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(epochs, h.get('val_ap50', []), color='#00BFA5', linewidth=2.5, label='mAP@0.5')\n",
    "    ax.plot(epochs, h.get('val_map', []), color='#00897B', linewidth=2.5, label='mAP@0.5:0.95')\n",
    "    \n",
    "    ax.set_title('Mean Average Precision (mAP)', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch', fontsize=14)\n",
    "    ax.set_ylabel('mAP', fontsize=14)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.set_ylim([0, 1.02])\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='lower right', frameon=True, fontsize=11)\n",
    "    \n",
    "    # --- 3. Precision and Recall ---\n",
    "    ax = axes[1, 0]\n",
    "    # Using AP50 as Precision Proxy (per instructions) and AR100 as Recall\n",
    "    ax.plot(epochs, h.get('val_ap50', []), color='#536DFE', linewidth=2.5, label='Precision (AP@0.50)')\n",
    "    ax.plot(epochs, h.get('val_ar_100', []), color='#7C4DFF', linewidth=2.5, label='Recall (AR@100)')\n",
    "    \n",
    "    ax.set_title('Precision and Recall', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch', fontsize=14)\n",
    "    ax.set_ylabel('Score', fontsize=14)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.set_ylim([0, 1.02])\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='lower right', frameon=True, fontsize=11)\n",
    "    \n",
    "    # --- 4. Summary Box ---\n",
    "    ax = axes[1, 1]\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Calculate stats\n",
    "    best_map_idx = np.argmax(h.get('val_map', [0]))\n",
    "    best_map = h['val_map'][best_map_idx]\n",
    "    final_map = h['val_map'][-1]\n",
    "    final_ap50 = h['val_ap50'][-1]\n",
    "    final_recall = h['val_ar_100'][-1]\n",
    "    start_loss = h['train_loss'][0]\n",
    "    end_loss = h['train_loss'][-1]\n",
    "    \n",
    "    summary_text = (\n",
    "        f\"Training Summary\\n\"\n",
    "        f\"=======================================\\n\\n\"\n",
    "        f\"Total Epochs: {total_epochs}\\n\\n\"\n",
    "        f\"Final Metrics (Epoch {total_epochs}):\\n\"\n",
    "        f\"  • mAP@0.5:        {final_ap50:.4f} ({final_ap50*100:.1f}%)\\n\"\n",
    "        f\"  • mAP@0.5:0.95:   {final_map:.4f} ({final_map*100:.1f}%)\\n\"\n",
    "        f\"  • Precision:      {final_ap50:.4f} ({final_ap50*100:.1f}%)\\n\"\n",
    "        f\"  • Recall:         {final_recall:.4f} ({final_recall*100:.1f}%)\\n\\n\"\n",
    "        f\"Best Performance:\\n\"\n",
    "        f\"  • Best mAP@0.5:0.95: {best_map:.4f} ({best_map*100:.1f}%)\\n\"\n",
    "        f\"  • Achieved at Epoch: {best_map_idx + 1}\\n\\n\"\n",
    "        f\"Loss Reduction:\\n\"\n",
    "        f\"  • Total Train Loss:  {start_loss:.4f} → {end_loss:.4f}\"\n",
    "    )\n",
    "    \n",
    "    # Add text box\n",
    "    props = dict(boxstyle='round', facecolor='#F5F5DC', alpha=0.5)\n",
    "    ax.text(0.1, 0.5, summary_text, transform=ax.transAxes, fontsize=14,\n",
    "            verticalalignment='center', bbox=props, fontfamily='monospace')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig('rcnn_learning_curves.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "plot_rcnn_learning_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Counting Error Analysis\n",
    "We run inference on the TEST set for both models, calculating errors per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on 99 test images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:20<00:00,  4.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 99 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def run_comparative_inference():\n",
    "    # Load Test Data Ground Truth from JSON\n",
    "    with open(RCNN_TEST_JSON, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    # Map image ID to filename and annotations\n",
    "    img_map = {img['id']: img for img in coco_data['images']}\n",
    "    ann_map = {}\n",
    "    for ann in coco_data['annotations']:\n",
    "        img_id = ann['image_id']\n",
    "        if img_id not in ann_map:\n",
    "            ann_map[img_id] = []\n",
    "        ann_map[img_id].append(ann)\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Running inference on {len(img_map)} test images...\")\n",
    "    \n",
    "    for img_id, img_info in tqdm(img_map.items()):\n",
    "        file_name = img_info['file_name']\n",
    "        img_path = RCNN_TEST_IMG_DIR / file_name\n",
    "        \n",
    "        if not img_path.exists():\n",
    "            continue\n",
    "            \n",
    "        # Ground Truth Count\n",
    "        gt_anns = ann_map.get(img_id, [])\n",
    "        gt_count = len(gt_anns)\n",
    "        \n",
    "        try:\n",
    "            pil_img = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            # YOLO Prediction\n",
    "            yolo_preds = yolo_model.predict(pil_img, conf_thres=CONF_THRESHOLD)\n",
    "            yolo_count = len(yolo_preds)\n",
    "            \n",
    "            # RCNN Prediction\n",
    "            rcnn_preds = rcnn_model.predict(pil_img, conf_thres=CONF_THRESHOLD)\n",
    "            rcnn_count = len(rcnn_preds)\n",
    "            \n",
    "            results.append({\n",
    "                'image': file_name,\n",
    "                'gt_count': gt_count,\n",
    "                'yolo_count': yolo_count,\n",
    "                'rcnn_count': rcnn_count,\n",
    "                'yolo_error': yolo_count - gt_count,\n",
    "                'rcnn_error': rcnn_count - gt_count\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "df_results = run_comparative_inference()\n",
    "print(f\"Processed {len(df_results)} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_comparison(df):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Scatter plot: X=Ground Truth Count, Y=Error (Predicted - GT)\n",
    "    plt.scatter(df['gt_count'], df['yolo_error'], c='blue', alpha=0.6, label='YOLOv9 Error', marker='o')\n",
    "    plt.scatter(df['gt_count'], df['rcnn_error'], c='red', alpha=0.6, label='R-CNN Error', marker='x')\n",
    "    \n",
    "    plt.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "    plt.title('Counting Error Comparison: Ground Truth vs Prediction Error', fontsize=18, fontweight='bold')\n",
    "    plt.xlabel('Ground Truth Count (Number of Cars)', fontsize=15)\n",
    "    plt.ylabel('Prediction Error (Pred - GT)', fontsize=15)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=13)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comparison_error_scatter.png')\n",
    "    plt.show()\n",
    "\n",
    "plot_error_comparison(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics\n",
    "Calculation of Mean Error and Standard Deviation for the summary text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPARISON SUMMARY\n",
      "==================\n",
      "Total Images Tested: 99\n",
      "\n",
      "YOLOv9 Performance:\n",
      "  Mean Absolute Error (MAE): 1.02\n",
      "  Error Distribution: 0.41 +/- 1.69\n",
      "\n",
      "Faster R-CNN Performance:\n",
      "  Mean Absolute Error (MAE): 3.64\n",
      "  Error Distribution: 3.62 +/- 2.93\n",
      "\n",
      "Conclusion:\n",
      "  YOLOv9 provided more accurate counts on average.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_summary(df):\n",
    "    yolo_mae = df['yolo_error'].abs().mean()\n",
    "    rcnn_mae = df['rcnn_error'].abs().mean()\n",
    "    \n",
    "    yolo_mean_err = df['yolo_error'].mean()\n",
    "    rcnn_mean_err = df['rcnn_error'].mean()\n",
    "    \n",
    "    yolo_std_err = df['yolo_error'].std()\n",
    "    rcnn_std_err = df['rcnn_error'].std()\n",
    "    \n",
    "    summary = \"COMPARISON SUMMARY\\n\"\n",
    "    summary += \"==================\\n\"\n",
    "    summary += f\"Total Images Tested: {len(df)}\\n\\n\"\n",
    "    \n",
    "    summary += \"YOLOv9 Performance:\\n\"\n",
    "    summary += f\"  Mean Absolute Error (MAE): {yolo_mae:.2f}\\n\"\n",
    "    summary += f\"  Error Distribution: {yolo_mean_err:.2f} +/- {yolo_std_err:.2f}\\n\\n\"\n",
    "    \n",
    "    summary += \"Faster R-CNN Performance:\\n\"\n",
    "    summary += f\"  Mean Absolute Error (MAE): {rcnn_mae:.2f}\\n\"\n",
    "    summary += f\"  Error Distribution: {rcnn_mean_err:.2f} +/- {rcnn_std_err:.2f}\\n\\n\"\n",
    "    \n",
    "    summary += \"Conclusion:\\n\"\n",
    "    if yolo_mae < rcnn_mae:\n",
    "        summary += \"  YOLOv9 provided more accurate counts on average.\\n\"\n",
    "    else:\n",
    "        summary += \"  Faster R-CNN provided more accurate counts on average.\\n\"\n",
    "        \n",
    "    print(summary)\n",
    "    \n",
    "    with open('model_comparison_summary.txt', 'w') as f:\n",
    "        f.write(summary)\n",
    "\n",
    "generate_summary(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visual Side-by-Side Comparison\n",
    "Visualizing predictions side-by-side: Ground Truth vs YOLOv9 vs Faster R-CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualization to comparison_sample_1.png\n",
      "Saved visualization to comparison_sample_2.png\n",
      "Saved visualization to comparison_sample_3.png\n"
     ]
    }
   ],
   "source": [
    "def draw_boxes(img_pil, boxes, color='red', width=3):\n",
    "    img = img_pil.copy()\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for box in boxes:\n",
    "        # Box format can be [x, y, w, h] (coco) or [x1, y1, x2, y2] (pascal)\n",
    "        # We need to handle both. Our models output [x1, y1, x2, y2]\n",
    "        # COCO JSON is [x, y, w, h]\n",
    "        if len(box) == 4:\n",
    "            # Determine if it's likely xywh or xyxy by checking if x2 < x1 is impossible\n",
    "            # For drawing, we assume input `boxes` are already in [x1, y1, x2, y2] format\n",
    "            # except for ground truth which we'll convert before calling this\n",
    "            draw.rectangle(box, outline=color, width=width)\n",
    "    return img\n",
    "\n",
    "def get_ground_truth_boxes(image_filename):\n",
    "    # Reload JSON to be safe\n",
    "    with open(RCNN_TEST_JSON, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    # Find image ID\n",
    "    img_id = None\n",
    "    for img in coco_data['images']:\n",
    "        if img['file_name'] == image_filename:\n",
    "            img_id = img['id']\n",
    "            break\n",
    "            \n",
    "    if img_id is None: return []\n",
    "    \n",
    "    boxes = []\n",
    "    for ann in coco_data['annotations']:\n",
    "        if ann['image_id'] == img_id:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            # Convert xywh to xyxy\n",
    "            boxes.append([x, y, x+w, y+h])\n",
    "    return boxes\n",
    "\n",
    "def visualize_side_by_side(num_samples=3):\n",
    "    # Pick random samples from our results dataframe\n",
    "    sample_df = df_results.sample(n=num_samples)\n",
    "    \n",
    "    for i, (_, row) in enumerate(sample_df.iterrows()):\n",
    "        img_filename = row['image']\n",
    "        img_path = RCNN_TEST_IMG_DIR / img_filename\n",
    "        pil_img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # 1. Ground Truth\n",
    "        gt_boxes = get_ground_truth_boxes(img_filename)\n",
    "        img_gt = draw_boxes(pil_img, gt_boxes, color='#00FF00') # Green\n",
    "        \n",
    "        # 2. YOLO Prediction\n",
    "        yolo_raw = yolo_model.predict(pil_img, conf_thres=CONF_THRESHOLD)\n",
    "        yolo_boxes = [p['box'] for p in yolo_raw]\n",
    "        img_yolo = draw_boxes(pil_img, yolo_boxes, color='#00FFFF') # Cyan\n",
    "        \n",
    "        # 3. RCNN Prediction\n",
    "        rcnn_raw = rcnn_model.predict(pil_img, conf_thres=CONF_THRESHOLD)\n",
    "        rcnn_boxes = [p['box'] for p in rcnn_raw]\n",
    "        img_rcnn = draw_boxes(pil_img, rcnn_boxes, color='#FF00FF') # Magenta\n",
    "        \n",
    "        # Plot - 3 Columns\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 8))\n",
    "        \n",
    "        ax1.imshow(img_gt)\n",
    "        ax1.set_title(f\"Ground Truth\\nCount: {len(gt_boxes)}\", fontsize=20, fontweight='bold')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        ax2.imshow(img_yolo)\n",
    "        ax2.set_title(f\"YOLOv9 Prediction\\nCount: {len(yolo_boxes)} (Diff: {len(yolo_boxes)-len(gt_boxes)})\", fontsize=20, fontweight='bold')\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        ax3.imshow(img_rcnn)\n",
    "        ax3.set_title(f\"Faster R-CNN Prediction\\nCount: {len(rcnn_boxes)} (Diff: {len(rcnn_boxes)-len(gt_boxes)})\", fontsize=20, fontweight='bold')\n",
    "        ax3.axis('off')\n",
    "        \n",
    "        # SAVE TO FILE with a unique name\n",
    "        save_path = f'comparison_sample_{i+1}.png'\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Saved visualization to {save_path}\")\n",
    "        plt.show()\n",
    "\n",
    "visualize_side_by_side(num_samples=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
