{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CARPK Parking Dataset Preparation\n",
    "\n",
    "This notebook prepares the CARPK dataset from HuggingFace for object detection training.\n",
    "\n",
    "## Notebook Structure\n",
    "1. Environment Setup and Imports\n",
    "2. Dataset Configuration\n",
    "3. Data Loading and Inspection\n",
    "4. Bounding Box Processing\n",
    "5. Dataset Splitting and Processing\n",
    "6. COCO Format Conversion\n",
    "7. Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from io import BytesIO\n",
    "\n",
    "print(\"All required packages imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "config = {\n",
    "    'output_dir': './data',\n",
    "    'train_split': 0.7,\n",
    "    'val_split': 0.2,\n",
    "    'test_split': 0.1,\n",
    "    'min_box_size': 3,\n",
    "    'max_samples': None,  # Set to a number for testing, None for full dataset\n",
    "    'image_quality': 95\n",
    "}\n",
    "\n",
    "# Initialize statistics tracking\n",
    "stats = {\n",
    "    'train': {'images': 0, 'boxes': 0, 'tiny_boxes': 0, 'invalid_boxes': 0},\n",
    "    'val': {'images': 0, 'boxes': 0, 'tiny_boxes': 0, 'invalid_boxes': 0},\n",
    "    'test': {'images': 0, 'boxes': 0, 'tiny_boxes': 0, 'invalid_boxes': 0}\n",
    "}\n",
    "\n",
    "box_sizes = []\n",
    "bbox_format = None\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(config['output_dir'])\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    (output_dir / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Created directory structure at {output_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading dataset from HuggingFace...\")\n",
    "dataset = load_dataset(\"backseollgi/parking_dataset\", \"carpk\", streaming=False)\n",
    "\n",
    "print(f\"Dataset loaded successfully\")\n",
    "print(f\"Available splits: {list(dataset.keys())}\")\n",
    "\n",
    "# Get samples\n",
    "if 'train' in dataset:\n",
    "    samples = list(dataset['train'])\n",
    "else:\n",
    "    key = list(dataset.keys())[0]\n",
    "    print(f\"Using '{key}' split instead of 'train'\")\n",
    "    samples = list(dataset[key])\n",
    "\n",
    "total_samples = len(samples)\n",
    "print(f\"Found {total_samples} samples\")\n",
    "\n",
    "if config['max_samples']:\n",
    "    total_samples = min(total_samples, config['max_samples'])\n",
    "    samples = samples[:total_samples]\n",
    "    print(f\"Limited to {total_samples} samples for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect First Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inspecting first sample...\")\n",
    "sample = samples[0]\n",
    "\n",
    "print(f\"Keys: {list(sample.keys())}\")\n",
    "print(f\"Image type: {type(sample['image'])}\")\n",
    "\n",
    "# Check for bounding boxes\n",
    "if 'bboxes' in sample:\n",
    "    print(f\"Number of bboxes: {len(sample['bboxes'])}\")\n",
    "    if len(sample['bboxes']) > 0:\n",
    "        print(f\"First bbox: {sample['bboxes'][0]}\")\n",
    "elif 'bbox' in sample:\n",
    "    print(f\"Number of bboxes: {len(sample['bbox'])}\")\n",
    "    if len(sample['bbox']) > 0:\n",
    "        print(f\"First bbox: {sample['bbox'][0]}\")\n",
    "else:\n",
    "    print(\"Warning: No 'bboxes' or 'bbox' field found\")\n",
    "\n",
    "# Check labels\n",
    "if 'labels' in sample:\n",
    "    print(f\"Labels field exists\")\n",
    "elif 'label' in sample:\n",
    "    print(f\"Label field exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bounding Box Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_bbox_format(bbox, img_width, img_height):\n",
    "    '''\n",
    "    Auto-detect bounding box format\n",
    "    Returns: 'xyxy' or 'xywh'\n",
    "    '''\n",
    "    global bbox_format\n",
    "    \n",
    "    if bbox_format is not None:\n",
    "        return bbox_format\n",
    "    \n",
    "    x1, y1, x2_or_w, y2_or_h = bbox\n",
    "    \n",
    "    if x2_or_w < x1 or y2_or_h < y1:\n",
    "        bbox_format = 'xywh'\n",
    "    elif x2_or_w > img_width or y2_or_h > img_height:\n",
    "        bbox_format = 'xywh'\n",
    "    else:\n",
    "        bbox_format = 'xyxy'\n",
    "    \n",
    "    print(f\"Detected bounding box format: {bbox_format}\")\n",
    "    return bbox_format\n",
    "\n",
    "def normalize_bbox(bbox, img_width, img_height):\n",
    "    '''\n",
    "    Convert bbox to [x1, y1, x2, y2] format\n",
    "    '''\n",
    "    fmt = detect_bbox_format(bbox, img_width, img_height)\n",
    "    \n",
    "    if fmt == 'xywh':\n",
    "        x, y, w, h = bbox\n",
    "        return [x, y, x + w, y + h]\n",
    "    else:\n",
    "        return list(bbox)\n",
    "\n",
    "def validate_and_clean_bbox(bbox, img_width, img_height, min_size=3):\n",
    "    '''\n",
    "    Validate and clean bounding box\n",
    "    Returns: (is_valid, cleaned_bbox, reason)\n",
    "    '''\n",
    "    try:\n",
    "        bbox = normalize_bbox(bbox, img_width, img_height)\n",
    "    except Exception as e:\n",
    "        return False, None, f\"normalization_error: {e}\"\n",
    "    \n",
    "    x1, y1, x2, y2 = bbox\n",
    "    \n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return False, None, \"invalid_coordinates\"\n",
    "    \n",
    "    # Clip to image boundaries\n",
    "    x1 = max(0, min(x1, img_width - 1))\n",
    "    y1 = max(0, min(y1, img_height - 1))\n",
    "    x2 = max(0, min(x2, img_width))\n",
    "    y2 = max(0, min(y2, img_height))\n",
    "    \n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return False, None, \"invalid_after_clipping\"\n",
    "    \n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    \n",
    "    if width < min_size or height < min_size:\n",
    "        return False, None, \"too_small\"\n",
    "    \n",
    "    box_sizes.append((width, height))\n",
    "    \n",
    "    return True, [float(x1), float(y1), float(x2), float(y2)], \"valid\"\n",
    "\n",
    "print(\"Bounding box processing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_normalize_image(image):\n",
    "    '''\n",
    "    Load image from various formats and convert to RGB numpy array\n",
    "    '''\n",
    "    try:\n",
    "        if isinstance(image, str):\n",
    "            image = Image.open(image).convert('RGB')\n",
    "            return np.array(image)\n",
    "        \n",
    "        elif isinstance(image, Image.Image):\n",
    "            image = image.convert('RGB')\n",
    "            return np.array(image)\n",
    "        \n",
    "        elif isinstance(image, np.ndarray):\n",
    "            if len(image.shape) == 2:\n",
    "                image = np.stack([image] * 3, axis=-1)\n",
    "            elif image.shape[2] == 4:\n",
    "                image = image[:, :, :3]\n",
    "            return image\n",
    "        \n",
    "        elif isinstance(image, bytes):\n",
    "            image = Image.open(BytesIO(image)).convert('RGB')\n",
    "            return np.array(image)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported image type: {type(image)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load image: {e}\")\n",
    "\n",
    "print(\"Image loading function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Splitting and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_name(index, total_samples):\n",
    "    '''\n",
    "    Determine which split a sample belongs to\n",
    "    '''\n",
    "    train_end = int(total_samples * config['train_split'])\n",
    "    val_end = int(total_samples * (config['train_split'] + config['val_split']))\n",
    "    \n",
    "    if index < train_end:\n",
    "        return 'train'\n",
    "    elif index < val_end:\n",
    "        return 'val'\n",
    "    else:\n",
    "        return 'test'\n",
    "\n",
    "def process_sample(sample, sample_idx, split, annotation_id):\n",
    "    '''\n",
    "    Process a single sample with validation\n",
    "    Returns: (image_info, annotations, new_annotation_id)\n",
    "    '''\n",
    "    try:\n",
    "        # Load and normalize image\n",
    "        image_np = load_and_normalize_image(sample['image'])\n",
    "        img_height, img_width = image_np.shape[:2]\n",
    "        \n",
    "        # Generate filename and save\n",
    "        img_filename = f\"{split}_{sample_idx:06d}.jpg\"\n",
    "        img_path = output_dir / split / img_filename\n",
    "        Image.fromarray(image_np).save(img_path, quality=config['image_quality'])\n",
    "        \n",
    "        # Get bounding boxes\n",
    "        bboxes = sample.get('bboxes', sample.get('bbox', []))\n",
    "        if not isinstance(bboxes, list):\n",
    "            bboxes = list(bboxes)\n",
    "        \n",
    "        # Get labels\n",
    "        labels = sample.get('labels', sample.get('label', [0] * len(bboxes)))\n",
    "        if not isinstance(labels, list):\n",
    "            labels = list(labels)\n",
    "        \n",
    "        if len(labels) < len(bboxes):\n",
    "            labels.extend([0] * (len(bboxes) - len(labels)))\n",
    "        \n",
    "        annotations = []\n",
    "        invalid_count = 0\n",
    "        tiny_count = 0\n",
    "        \n",
    "        for bbox, label in zip(bboxes, labels):\n",
    "            if bbox is None or len(bbox) != 4:\n",
    "                invalid_count += 1\n",
    "                continue\n",
    "            \n",
    "            is_valid, cleaned_bbox, reason = validate_and_clean_bbox(\n",
    "                bbox, img_width, img_height, config['min_box_size']\n",
    "            )\n",
    "            \n",
    "            if not is_valid:\n",
    "                if reason == \"too_small\":\n",
    "                    tiny_count += 1\n",
    "                else:\n",
    "                    invalid_count += 1\n",
    "                continue\n",
    "            \n",
    "            x1, y1, x2, y2 = cleaned_bbox\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            \n",
    "            # COCO format: [x, y, width, height]\n",
    "            coco_bbox = [x1, y1, width, height]\n",
    "            \n",
    "            ann = {\n",
    "                'id': annotation_id,\n",
    "                'image_id': sample_idx,\n",
    "                'category_id': 1,\n",
    "                'bbox': coco_bbox,\n",
    "                'area': float(width * height),\n",
    "                'iscrowd': 0\n",
    "            }\n",
    "            annotations.append(ann)\n",
    "            annotation_id += 1\n",
    "        \n",
    "        # Skip images with no valid annotations\n",
    "        if len(annotations) == 0:\n",
    "            os.remove(img_path)\n",
    "            return None, None, annotation_id\n",
    "        \n",
    "        # Update statistics\n",
    "        stats[split]['images'] += 1\n",
    "        stats[split]['boxes'] += len(annotations)\n",
    "        stats[split]['tiny_boxes'] += tiny_count\n",
    "        stats[split]['invalid_boxes'] += invalid_count\n",
    "        \n",
    "        image_info = {\n",
    "            'id': sample_idx,\n",
    "            'file_name': img_filename,\n",
    "            'width': img_width,\n",
    "            'height': img_height\n",
    "        }\n",
    "        \n",
    "        return image_info, annotations, annotation_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample {sample_idx}: {e}\")\n",
    "        return None, None, annotation_id\n",
    "\n",
    "print(\"Processing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process All Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing samples...\\n\")\n",
    "\n",
    "train_images, train_annotations = [], []\n",
    "val_images, val_annotations = [], []\n",
    "test_images, test_annotations = [], []\n",
    "\n",
    "train_ann_id = 1\n",
    "val_ann_id = 1\n",
    "test_ann_id = 1\n",
    "\n",
    "for idx in tqdm(range(total_samples), desc=\"Processing\"):\n",
    "    sample = samples[idx]\n",
    "    split = get_split_name(idx, total_samples)\n",
    "    \n",
    "    if split == 'train':\n",
    "        img_info, anns, train_ann_id = process_sample(\n",
    "            sample, idx, split, train_ann_id\n",
    "        )\n",
    "        if img_info is not None:\n",
    "            train_images.append(img_info)\n",
    "            train_annotations.extend(anns)\n",
    "            \n",
    "    elif split == 'val':\n",
    "        img_info, anns, val_ann_id = process_sample(\n",
    "            sample, idx, split, val_ann_id\n",
    "        )\n",
    "        if img_info is not None:\n",
    "            val_images.append(img_info)\n",
    "            val_annotations.extend(anns)\n",
    "            \n",
    "    else:\n",
    "        img_info, anns, test_ann_id = process_sample(\n",
    "            sample, idx, split, test_ann_id\n",
    "        )\n",
    "        if img_info is not None:\n",
    "            test_images.append(img_info)\n",
    "            test_annotations.extend(anns)\n",
    "\n",
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Train images: {len(train_images)}\")\n",
    "print(f\"Val images: {len(val_images)}\")\n",
    "print(f\"Test images: {len(test_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. COCO Format Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coco_json(split, images, annotations):\n",
    "    '''\n",
    "    Create COCO format JSON file\n",
    "    '''\n",
    "    coco_format = {\n",
    "        'images': images,\n",
    "        'annotations': annotations,\n",
    "        'categories': [\n",
    "            {\n",
    "                'id': 1,\n",
    "                'name': 'car',\n",
    "                'supercategory': 'vehicle'\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    json_path = output_dir / f'{split}.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(coco_format, f, indent=2)\n",
    "    \n",
    "    print(f\"Created {json_path}\")\n",
    "    print(f\"  Images: {len(images)}\")\n",
    "    print(f\"  Annotations: {len(annotations)}\")\n",
    "\n",
    "print(\"Creating COCO format files...\\n\")\n",
    "\n",
    "if len(train_images) > 0:\n",
    "    create_coco_json('train', train_images, train_annotations)\n",
    "else:\n",
    "    print(\"No training images processed\")\n",
    "\n",
    "if len(val_images) > 0:\n",
    "    create_coco_json('val', val_images, val_annotations)\n",
    "else:\n",
    "    print(\"No validation images processed\")\n",
    "\n",
    "if len(test_images) > 0:\n",
    "    create_coco_json('test', test_images, test_annotations)\n",
    "else:\n",
    "    print(\"No test images processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Dataset Statistics\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_images = sum(s['images'] for s in stats.values())\n",
    "total_boxes = sum(s['boxes'] for s in stats.values())\n",
    "\n",
    "if total_images == 0:\n",
    "    print(\"\\nNo images were processed successfully!\")\n",
    "else:\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_stats = stats[split]\n",
    "        imgs = split_stats['images']\n",
    "        \n",
    "        if imgs == 0:\n",
    "            continue\n",
    "        \n",
    "        boxes = split_stats['boxes']\n",
    "        tiny = split_stats['tiny_boxes']\n",
    "        invalid = split_stats['invalid_boxes']\n",
    "        avg_boxes = boxes / imgs if imgs > 0 else 0\n",
    "        \n",
    "        print(f\"\\n{split.upper()}:\")\n",
    "        print(f\"  Images: {imgs} ({imgs/total_images*100:.1f}%)\")\n",
    "        print(f\"  Valid boxes: {boxes}\")\n",
    "        print(f\"  Avg boxes/image: {avg_boxes:.2f}\")\n",
    "        print(f\"  Rejected (too small): {tiny}\")\n",
    "        print(f\"  Rejected (invalid): {invalid}\")\n",
    "    \n",
    "    print(f\"\\nTOTAL:\")\n",
    "    print(f\"  Images: {total_images}\")\n",
    "    print(f\"  Valid boxes: {total_boxes}\")\n",
    "    print(f\"  Avg boxes/image: {total_boxes/total_images:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding Box Size Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(box_sizes) > 0:\n",
    "    widths = [w for w, h in box_sizes]\n",
    "    heights = [h for w, h in box_sizes]\n",
    "    areas = [w * h for w, h in box_sizes]\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"Bounding Box Size Analysis\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nWidth Statistics:\")\n",
    "    print(f\"  Min: {np.min(widths):.1f}px\")\n",
    "    print(f\"  Max: {np.max(widths):.1f}px\")\n",
    "    print(f\"  Mean: {np.mean(widths):.1f}px\")\n",
    "    print(f\"  Median: {np.median(widths):.1f}px\")\n",
    "    print(f\"  25th percentile: {np.percentile(widths, 25):.1f}px\")\n",
    "    print(f\"  75th percentile: {np.percentile(widths, 75):.1f}px\")\n",
    "    \n",
    "    print(f\"\\nHeight Statistics:\")\n",
    "    print(f\"  Min: {np.min(heights):.1f}px\")\n",
    "    print(f\"  Max: {np.max(heights):.1f}px\")\n",
    "    print(f\"  Mean: {np.mean(heights):.1f}px\")\n",
    "    print(f\"  Median: {np.median(heights):.1f}px\")\n",
    "    print(f\"  25th percentile: {np.percentile(heights, 25):.1f}px\")\n",
    "    print(f\"  75th percentile: {np.percentile(heights, 75):.1f}px\")\n",
    "    \n",
    "    print(f\"\\nArea Statistics:\")\n",
    "    print(f\"  Min: {np.min(areas):.1f}px²\")\n",
    "    print(f\"  Max: {np.max(areas):.1f}px²\")\n",
    "    print(f\"  Mean: {np.mean(areas):.1f}px²\")\n",
    "    print(f\"  Median: {np.median(areas):.1f}px²\")\n",
    "    \n",
    "    # Recommend anchor sizes\n",
    "    sqrt_areas = np.sqrt(areas)\n",
    "    percentiles = [10, 30, 50, 70, 90]\n",
    "    anchor_sizes = [int(np.percentile(sqrt_areas, p)) for p in percentiles]\n",
    "    \n",
    "    print(\"\\nRecommended Anchor Sizes:\")\n",
    "    print(f\"  {tuple(anchor_sizes)}\")\n",
    "    print(f\"  (Based on {percentiles}th percentiles of sqrt(area))\")\n",
    "    print(f\"\\nUse this when training:\")\n",
    "    print(f\"  --anchor-sizes \\\"{','.join(map(str, anchor_sizes))}\\\"\")\n",
    "else:\n",
    "    print(\"No box sizes to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(split, num_samples=3):\n",
    "    '''\n",
    "    Visualize samples to verify data quality\n",
    "    '''\n",
    "    json_path = output_dir / f'{split}.json'\n",
    "    \n",
    "    if not json_path.exists():\n",
    "        print(f\"Cannot visualize {split}: JSON file not found\")\n",
    "        return\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if len(data['images']) == 0:\n",
    "        print(f\"No images in {split} split to visualize\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nVisualizing {num_samples} samples from {split}...\")\n",
    "    \n",
    "    num_samples = min(num_samples, len(data['images']))\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 5))\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx in range(num_samples):\n",
    "        img_info = data['images'][idx]\n",
    "        img_path = output_dir / split / img_info['file_name']\n",
    "        \n",
    "        if not img_path.exists():\n",
    "            print(f\"Image not found: {img_path}\")\n",
    "            continue\n",
    "        \n",
    "        img = Image.open(img_path)\n",
    "        img_anns = [ann for ann in data['annotations'] if ann['image_id'] == img_info['id']]\n",
    "        \n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(f\"{split} - {len(img_anns)} boxes\")\n",
    "        axes[idx].axis('off')\n",
    "        \n",
    "        for ann in img_anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            rect = patches.Rectangle(\n",
    "                (x, y), w, h,\n",
    "                linewidth=2,\n",
    "                edgecolor='red',\n",
    "                facecolor='none'\n",
    "            )\n",
    "            axes[idx].add_patch(rect)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = output_dir / f'{split}_samples.png'\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Saved visualization to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize train and val splits\n",
    "for split in ['train', 'val']:\n",
    "    if stats[split]['images'] > 0:\n",
    "        visualize_samples(split, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Dataset preparation is complete. The data is now ready for training.\n",
    "\n",
    "### Next Steps\n",
    "1. Review the visualization images above\n",
    "2. Check the statistics to ensure data quality\n",
    "3. Note the recommended anchor sizes for training\n",
    "4. Proceed to train your object detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Dataset Preparation Complete\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nData saved to: {output_dir.absolute()}\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(f\"  - train.json ({len(train_images)} images)\")\n",
    "print(f\"  - val.json ({len(val_images)} images)\")\n",
    "print(f\"  - test.json ({len(test_images)} images)\")\n",
    "print(f\"  - train/ directory with {len(train_images)} images\")\n",
    "print(f\"  - val/ directory with {len(val_images)} images\")\n",
    "print(f\"  - test/ directory with {len(test_images)} images\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}