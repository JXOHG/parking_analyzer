{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv9 Model Evaluation\n",
    "\n",
    "Comprehensive evaluation of YOLOv9 model performance on parking lot detection task.\n",
    "\n",
    "**Metrics Evaluated:**\n",
    "- Object Detection: mAP@0.5, mAP@0.5:0.95, Precision, Recall\n",
    "- Counting Accuracy: Exact Match Accuracy, Mean Absolute Error (MAE)\n",
    "- Inference Speed: Frames Per Second (FPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Apply YOLOv9 Patch\n",
    "\n",
    "Fix AttributeError in detect.py for models with list-based output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "YOLOv9_DIR = Path('./yolov9')\n",
    "detect_script_path = YOLOv9_DIR / 'detect.py'\n",
    "patch_signature = \"# PATCHED: Handle list output from model\"\n",
    "\n",
    "if detect_script_path.exists():\n",
    "    with open(detect_script_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    already_patched = any(patch_signature in line for line in lines)\n",
    "\n",
    "    if not already_patched:\n",
    "        print(f\"Applying patch to {detect_script_path}...\")\n",
    "        new_lines = []\n",
    "        for line in lines:\n",
    "            new_lines.append(line)\n",
    "            if 'pred = model(im, augment=augment, visualize=visualize)' in line:\n",
    "                indent = ' ' * (len(line) - len(line.lstrip()))\n",
    "                new_lines.append(f\"{indent}{patch_signature}\\n\")\n",
    "                new_lines.append(f\"{indent}if isinstance(pred, list):\\n\")\n",
    "                new_lines.append(f\"{indent}    pred = pred[0]\\n\")\n",
    "        \n",
    "        with open(detect_script_path, 'w', encoding='utf-8') as f:\n",
    "            f.writelines(new_lines)\n",
    "        print(\"Patch applied successfully.\")\n",
    "    else:\n",
    "        print(f\"{detect_script_path} is already patched.\")\n",
    "else:\n",
    "    print(f\"Warning: {detect_script_path} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import yaml\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "YOLO_RUN_DIR = Path('./runs/train/carpk_yolov9/')\n",
    "MODEL_CHECKPOINT = YOLO_RUN_DIR / 'weights' / 'best.pt'\n",
    "DATA_YAML = Path('./prepared_data/yolo/data.yaml')\n",
    "IMG_SIZE = 640\n",
    "CONF_THRESHOLD = 0.25\n",
    "IOU_THRESHOLD = 0.7\n",
    "DEVICE = '0'\n",
    "\n",
    "if not MODEL_CHECKPOINT.exists(): \n",
    "    raise FileNotFoundError(f\"Model checkpoint not found: {MODEL_CHECKPOINT}\")\n",
    "if not YOLOv9_DIR.exists() or not (YOLOv9_DIR / 'val.py').exists(): \n",
    "    raise FileNotFoundError(f\"YOLOv9 repository not found at: {YOLOv9_DIR}\")\n",
    "if not DATA_YAML.exists(): \n",
    "    raise FileNotFoundError(f\"Data YAML not found: {DATA_YAML}\")\n",
    "\n",
    "print(f\"Model: {MODEL_CHECKPOINT}\")\n",
    "print(f\"Data: {DATA_YAML}\")\n",
    "print(f\"Config: img={IMG_SIZE}, conf={CONF_THRESHOLD}, iou={IOU_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Object Detection Metrics (mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation():\n",
    "    original_dir = Path.cwd()\n",
    "    abs_weights = MODEL_CHECKPOINT.resolve()\n",
    "    abs_data_yaml = DATA_YAML.resolve()\n",
    "    os.chdir(YOLOv9_DIR)\n",
    "    \n",
    "    cmd = [\n",
    "        sys.executable, 'val.py',\n",
    "        '--data', str(abs_data_yaml),\n",
    "        '--weights', str(abs_weights),\n",
    "        '--img', str(IMG_SIZE),\n",
    "        '--conf-thres', '0.001',\n",
    "        '--iou-thres', str(IOU_THRESHOLD),\n",
    "        '--device', DEVICE,\n",
    "        '--task', 'test',\n",
    "        '--verbose'\n",
    "    ]\n",
    "    \n",
    "    print(\"Running validation...\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8')\n",
    "    os.chdir(original_dir)\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Validation failed with return code {result.returncode}\")\n",
    "        print(result.stderr)\n",
    "        return \"\"\n",
    "    \n",
    "    full_output = result.stdout + \"\\n\" + result.stderr\n",
    "    return full_output\n",
    "\n",
    "def parse_validation_output(output):\n",
    "    metrics = {}\n",
    "    \n",
    "    all_class_line = re.search(r\"all\\s+\\d+\\s+\\d+\\s+([\\d\\.]+)\\s+([\\d\\.]+)\\s+([\\d\\.]+)\\s+([\\d\\.]+)\", output)\n",
    "    if all_class_line:\n",
    "        metrics['precision'] = float(all_class_line.group(1))\n",
    "        metrics['recall'] = float(all_class_line.group(2))\n",
    "        metrics['map_50'] = float(all_class_line.group(3))\n",
    "        metrics['map_50_95'] = float(all_class_line.group(4))\n",
    "\n",
    "    speed_patterns = [\n",
    "        r\"Speed:.*?([\\d\\.]+)ms\\s+inference\",\n",
    "        r\"inference:\\s*([\\d\\.]+)ms\",\n",
    "        r\"([\\d\\.]+)ms.*?inference\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in speed_patterns:\n",
    "        speed_line = re.search(pattern, output, re.IGNORECASE)\n",
    "        if speed_line:\n",
    "            inference_ms = float(speed_line.group(1))\n",
    "            if inference_ms > 0: \n",
    "                metrics['fps'] = 1000.0 / inference_ms\n",
    "            break\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "validation_output = run_validation()\n",
    "bbox_metrics = parse_validation_output(validation_output)\n",
    "\n",
    "print(\"\\nDetection Metrics:\")\n",
    "for key, val in bbox_metrics.items():\n",
    "    print(f\"  {key}: {val:.4f}\" if key != 'fps' else f\"  {key}: {val:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Counting Metrics (Accuracy & MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_directories():\n",
    "    abs_data_yaml = DATA_YAML.resolve()\n",
    "    \n",
    "    with open(abs_data_yaml, 'r') as f:\n",
    "        data_config = yaml.safe_load(f)\n",
    "    \n",
    "    test_path_str = data_config.get('test', '')\n",
    "    if not test_path_str:\n",
    "        raise ValueError(\"'test' key not found in data.yaml\")\n",
    "    \n",
    "    test_path = (abs_data_yaml.parent / test_path_str).resolve()\n",
    "    \n",
    "    if test_path.name == 'images' and test_path.exists():\n",
    "        test_img_dir = test_path\n",
    "        gt_label_dir = test_path.parent / 'labels'\n",
    "    elif (test_path / 'images').exists():\n",
    "        test_img_dir = test_path / 'images'\n",
    "        gt_label_dir = test_path / 'labels'\n",
    "    else:\n",
    "        test_img_dir = test_path\n",
    "        gt_label_dir = test_path.with_name('labels')\n",
    "        if not gt_label_dir.exists():\n",
    "            gt_label_dir = test_path.parent / 'labels'\n",
    "    \n",
    "    return test_img_dir, gt_label_dir\n",
    "\n",
    "def deduplicate_predictions(pred_label_dir):\n",
    "    label_files = list(pred_label_dir.glob('*.txt'))\n",
    "    \n",
    "    total_before = 0\n",
    "    total_after = 0\n",
    "    \n",
    "    for label_file in label_files:\n",
    "        if not label_file.exists():\n",
    "            continue\n",
    "            \n",
    "        with open(label_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        total_before += len(lines)\n",
    "        \n",
    "        unique_lines = []\n",
    "        seen = set()\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line and line not in seen:\n",
    "                unique_lines.append(line)\n",
    "                seen.add(line)\n",
    "        \n",
    "        total_after += len(unique_lines)\n",
    "        \n",
    "        with open(label_file, 'w') as f:\n",
    "            f.write('\\n'.join(unique_lines) + '\\n' if unique_lines else '')\n",
    "    \n",
    "    if total_before > total_after:\n",
    "        print(f\"Removed {total_before - total_after} duplicate detections ({(total_before-total_after)/total_before*100:.1f}%)\")\n",
    "\n",
    "def run_detection_and_get_paths():\n",
    "    import shutil\n",
    "    \n",
    "    original_dir = Path.cwd()\n",
    "    abs_weights = MODEL_CHECKPOINT.resolve()\n",
    "    test_img_dir, _ = get_test_directories()\n",
    "\n",
    "    project_dir = original_dir / 'runs' / 'detect_for_counting'\n",
    "    exp_name = 'exp'\n",
    "    pred_label_dir = project_dir / exp_name / 'labels'\n",
    "    \n",
    "    if (project_dir / exp_name).exists():\n",
    "        shutil.rmtree(project_dir / exp_name)\n",
    "    \n",
    "    os.chdir(YOLOv9_DIR)\n",
    "    cmd = [\n",
    "        sys.executable, 'detect.py',\n",
    "        '--weights', str(abs_weights),\n",
    "        '--source', str(test_img_dir),\n",
    "        '--img', str(IMG_SIZE),\n",
    "        '--conf', str(CONF_THRESHOLD),\n",
    "        '--iou-thres', str(IOU_THRESHOLD),\n",
    "        '--device', DEVICE,\n",
    "        '--save-txt', '--project', str(project_dir), '--name', exp_name\n",
    "    ]\n",
    "    \n",
    "    print(\"Running detection...\")\n",
    "    \n",
    "    try:\n",
    "        subprocess.run(cmd, check=True, capture_output=True, text=True, encoding='utf-8')\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Detection failed: {e.stderr}\")\n",
    "        raise\n",
    "    finally:\n",
    "        os.chdir(original_dir)\n",
    "\n",
    "    deduplicate_predictions(pred_label_dir)\n",
    "    return pred_label_dir\n",
    "\n",
    "def calculate_counting_metrics(pred_label_dir):\n",
    "    test_img_dir, gt_label_dir = get_test_directories()\n",
    "    \n",
    "    if not gt_label_dir.exists():\n",
    "        raise FileNotFoundError(f\"Ground truth label directory not found: {gt_label_dir}\")\n",
    "    \n",
    "    test_images = list(test_img_dir.glob('*.jpg')) + list(test_img_dir.glob('*.png'))\n",
    "    if not test_images:\n",
    "        raise ValueError(f\"No images found in {test_img_dir}\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for img_path in tqdm(test_images, desc=\"Calculating counting metrics\"):\n",
    "        gt_label_path = gt_label_dir / f\"{img_path.stem}.txt\"\n",
    "        pred_label_path = pred_label_dir / f\"{img_path.stem}.txt\"\n",
    "\n",
    "        gt_count = 0\n",
    "        if gt_label_path.exists():\n",
    "            with open(gt_label_path, 'r') as f: \n",
    "                gt_count = sum(1 for line in f if line.strip())\n",
    "\n",
    "        pred_count = 0\n",
    "        if pred_label_path.exists():\n",
    "            with open(pred_label_path, 'r') as f: \n",
    "                pred_count = sum(1 for line in f if line.strip())\n",
    "        \n",
    "        results.append({\n",
    "            'image': img_path.name,\n",
    "            'gt_count': gt_count,\n",
    "            'pred_count': pred_count,\n",
    "            'difference': pred_count - gt_count,\n",
    "            'abs_error': abs(pred_count - gt_count)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    correct_counts = (df['difference'] == 0).sum()\n",
    "    total_images = len(df)\n",
    "    exact_accuracy = (correct_counts / total_images) * 100\n",
    "    mae = df['abs_error'].mean()\n",
    "    \n",
    "    return {'count_accuracy': exact_accuracy, 'count_mae': mae}, df\n",
    "\n",
    "pred_label_dir = run_detection_and_get_paths()\n",
    "counting_metrics, count_df = calculate_counting_metrics(pred_label_dir)\n",
    "\n",
    "print(\"\\nCounting Metrics:\")\n",
    "print(f\"  Exact Accuracy: {counting_metrics['count_accuracy']:.2f}%\")\n",
    "print(f\"  MAE: {counting_metrics['count_mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Count Analysis Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Ground Truth vs Predicted scatter plot\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(count_df['gt_count'], count_df['pred_count'], alpha=0.6, s=50)\n",
    "max_val = max(count_df['gt_count'].max(), count_df['pred_count'].max())\n",
    "ax1.plot([0, max_val], [0, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "ax1.set_xlabel('Ground Truth Count', fontsize=12)\n",
    "ax1.set_ylabel('Predicted Count', fontsize=12)\n",
    "ax1.set_title('Ground Truth vs Predicted Counts', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Error distribution histogram\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(count_df['difference'], bins=30, edgecolor='black', alpha=0.7)\n",
    "ax2.axvline(x=0, color='r', linestyle='--', linewidth=2, label='Zero Error')\n",
    "ax2.set_xlabel('Prediction Error (Pred - GT)', fontsize=12)\n",
    "ax2.set_ylabel('Frequency', fontsize=12)\n",
    "ax2.set_title('Distribution of Prediction Errors', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Count distribution comparison\n",
    "ax3 = axes[1, 0]\n",
    "bins = np.arange(0, max(count_df['gt_count'].max(), count_df['pred_count'].max()) + 2, 1)\n",
    "ax3.hist(count_df['gt_count'], bins=bins, alpha=0.5, label='Ground Truth', edgecolor='black')\n",
    "ax3.hist(count_df['pred_count'], bins=bins, alpha=0.5, label='Predicted', edgecolor='black')\n",
    "ax3.set_xlabel('Count', fontsize=12)\n",
    "ax3.set_ylabel('Frequency', fontsize=12)\n",
    "ax3.set_title('Distribution of Counts', fontsize=14, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Sample-by-sample comparison (first 30 images)\n",
    "ax4 = axes[1, 1]\n",
    "n_samples = min(30, len(count_df))\n",
    "x = np.arange(n_samples)\n",
    "width = 0.35\n",
    "ax4.bar(x - width/2, count_df['gt_count'].iloc[:n_samples], width, label='Ground Truth', alpha=0.8)\n",
    "ax4.bar(x + width/2, count_df['pred_count'].iloc[:n_samples], width, label='Predicted', alpha=0.8)\n",
    "ax4.set_xlabel('Image Index', fontsize=12)\n",
    "ax4.set_ylabel('Count', fontsize=12)\n",
    "ax4.set_title(f'Count Comparison (First {n_samples} Images)', fontsize=14, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nCount Analysis Statistics:\")\n",
    "print(f\"  Total images: {len(count_df)}\")\n",
    "print(f\"  Exact matches: {(count_df['difference'] == 0).sum()} ({(count_df['difference'] == 0).sum()/len(count_df)*100:.2f}%)\")\n",
    "print(f\"  Mean GT count: {count_df['gt_count'].mean():.2f}\")\n",
    "print(f\"  Mean Pred count: {count_df['pred_count'].mean():.2f}\")\n",
    "print(f\"  GT count range: {count_df['gt_count'].min()} - {count_df['gt_count'].max()}\")\n",
    "print(f\"  Pred count range: {count_df['pred_count'].min()} - {count_df['pred_count'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"         YOLOv9 Performance Summary\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if bbox_metrics and len(bbox_metrics) > 0:\n",
    "    print(f\" mAP@0.5:0.95:        {bbox_metrics.get('map_50_95', 0):.4f}\")\n",
    "    print(f\" mAP@0.5:             {bbox_metrics.get('map_50', 0):.4f}\") \n",
    "    print(f\" Precision:           {bbox_metrics.get('precision', 0):.4f}\")\n",
    "    print(f\" Recall:              {bbox_metrics.get('recall', 0):.4f}\")\n",
    "else:\n",
    "    print(\" Detection metrics unavailable\")\n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "if counting_metrics:\n",
    "    print(f\" Count Accuracy:      {counting_metrics.get('count_accuracy', 0):.2f}%\")\n",
    "    print(f\" Count MAE:           {counting_metrics.get('count_mae', 0):.4f}\")\n",
    "else:\n",
    "    print(\" Counting metrics unavailable\")\n",
    "    \n",
    "print(\"-\"*50)\n",
    "\n",
    "if bbox_metrics and bbox_metrics.get('fps'):\n",
    "    print(f\" Inference Speed:     {bbox_metrics.get('fps', 0):.2f} FPS\")\n",
    "else:\n",
    "    print(\" FPS metric unavailable\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(num_samples=5):\n",
    "    pred_dir = Path('./runs/detect_for_counting/exp/')\n",
    "    if not pred_dir.exists():\n",
    "        print(f\"Prediction directory not found: {pred_dir}\")\n",
    "        return\n",
    "        \n",
    "    pred_images = list(pred_dir.glob('*.jpg')) + list(pred_dir.glob('*.png'))\n",
    "    if not pred_images:\n",
    "        print(\"No prediction images found.\")\n",
    "        return\n",
    "    \n",
    "    sample_indices = np.random.choice(len(pred_images), min(num_samples, len(pred_images)), replace=False)\n",
    "    \n",
    "    for i in sample_indices:\n",
    "        img_path = pred_images[i]\n",
    "        img = Image.open(img_path)\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"{img_path.name}\", fontsize=14)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "visualize_predictions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
